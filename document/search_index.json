[
["index.html", "How the 2016 US Election Happened Introduction", " How the 2016 US Election Happened An Phi 2017-05-08 Introduction “Why!” exclaimed our friends in disappointment as we all witness Hillary Clinton losing Pennsylvania. To some extent, we could relate to them; we were also puzzled by this outcome: Hillary did swamp Donald Trump in the popular vote, but she lost the major battle. In fact, before the fateful night, most people believed that she was the “meant to be” and so the following morning, the result they would read on the newspapers would be exactly just that. But perhaps, politics at time can be quite unpredictable. Or is it? Instead of awing at the perplexity of the matter, perhaps, we can take a more mature and scientific approach to this by studying the 2016 election result and try to figure out ourselves how things actually happened. Nevertheless, all is tied to the word “perhaps”, and we might actually end up learning nothing about this subject matter. But let no pessimism haunt our path and let us move on to the real question “What do we wish to learn?” we have contemplated this question for quite a some time and there are three goals we want to achieve. First, we want to understand how various factors (demographic, geographic and even historical) influence the result. Based on this, we would need to collect data that correspond to the factors we mentioned, but unlike geographic and demographic data, historical data is really hard to be collected in mass, not to say to be quantified and classified in general. As such, we humbly limit our history factor to just recent past years’ election results (i.e 2008 and 2012). In terms of the granularity of the data set, we will use the results by county (that will yield us a suitable size for our data set). This choice also comes from the fact that despite the smallest election division being precinct, county is the smallest division which we can collect demographic and geographic data. But to nobody’s surprise, past election result can be a really strong predictor of the 2016 election outcome. As such, our second concern comes quite naturally: if we only consider demographic and geographic factors, can we actually build a model to predict the result for each county. Our concern is that whether or not the state where a county belongs to can affect the vote. Perhaps, including the state in the model might sounds dubious but because when we talk about election, we often say it in the form of “Mr./Mrs. won state X” and also because being a voter living in a state that has been voting for a single party for 50 years might affect a person’s choice just like how families, friends, and neighbors might affect your choice of vote, we feel that state is justified to be in the data set. As for demographic attributes, our view is that we will include common demographic factors such as: age, life expectancy, income inequality, race and ethnicity, etc. We also include crime and weather data, which at first glance might seem extremely unlikely to be useful in our model but we will be very vigilant in our feature selection step to make sure redundant and irrelevant features will not be included while we train our model. To follow the convention, we will take the CRISP-DM model approach. Of course, we can refer to the questions we are trying to solve as our “buisness goals” but we are not really trying to solve any business problem, we will not formally document our business understanding. Neither do we wish to deploy our learned model in the end to predict future election results (however, we will have a subsection dedicated to explaining what we learned that might prove helpful to political parties based on the models learned; in the CRISP-DM sense, this is part of the deployment procedure). As such, we will deliver our data quality report (DQR) to explain our attempts at getting to know the data, as well as our data quality plan (DQP) to provide detailed procedures of how we prepare the data and a report which succinctly sums up our approach and discusses in detail the learned models along with their performance evaluations and interpretations. We also include a short section dedicated to explaining the process of choosing the classifiers, preparation for tuning their parameters and pre-processing steps required for the data set prior to modelling. We name our report “A ‘Minority’ Report” with the intention to explicate how Donald Trump won the election despite the popular (majority) prediction, and so we hope that people who find the results unsettling and unfair (or even unjust) or people who seek relief via conspiracy theories will take a few minutes to see what numbers and figures have to say about this election. "],
["disclaimer.html", "Disclaimer", " Disclaimer Apprently, the election data provided by USA.county.data is scapred from the New York Times, who bought the data set from Associated Press. Since this might have complicated legal issue, we wish to express that we are not responsible for the election data as we are just taking it as is from the noted source. Also, since this is data-mining project, we wish to show the step through which we clean and transform the data set so that people can reproduce the results that we obtained, as such, it is almost impossible to not have the election data anywhere in this repository. For healthcare and human-development data sets, we also have to include them in our project because we use them for data preparation step. As such, we just really want to express that we use this data set merely for study purpose and have NO intention to redistribute, nor to commercialize it in anyway. So please do not cite us as the source of these data sets if you choose to use them yourselves. Please refer to the original sources of these data files as we have detailed in section 1.2. This project was built as part of our class project, all conclusion obtained in the paper are based on data-mining, figures and numbers. We do not want the results to be used in anyway to favor any party, i.e. we take a political-free and neutral stance while working on this project. Also, we deem ourselves as lacking both political understanding and data-mining related knowledge so the results obtained might just be taken lightly and should not be cited in any publications. "],
["dqr.html", "Section 1 Data Quality Report (DQR) 1.1 Overview 1.2 Data Source 1.3 Data Pre-Process 1.4 Data Exploration 1.5 Feature Detail 1.6 Deleted Feature", " Section 1 Data Quality Report (DQR) 1.1 Overview As mentioned, the data set we are using in this study consists of demographic, socioeconomic and geographic data by county. This data set is taken mainly from a source compiled by Emil O. W. Kirkegaard for usage in his socioeconomic research (Kirkegaard 2016). For general purpose, Emil has kindly tidied up his original data set and included the scraped election data of the year 2016, 2012, and 2008 from the New York Times as well as weather data by county, which is really tedious to collect, from the National Oceanic and Atmospheric Administration. Due to its extensive coverage of many demographic indicators, we decide to take this as our starting point from which we will add on more relevant data in order to obtain our Analytic Base Table (ABT). The number of counties we have complete election data (N = 3148) will limit the size of instances (counties) we have in the data set. The original data set contains a total of 161 features, many of which are derived features that serve the author’s research paper and many of them are irrelevant to our study goal. As such, prior to assessing and analyzing the quality of our data, we will need to pre-process the original data set, meaning removing irrelevant features and adding as well as deriving necessary features. In the usual CRISP-DM cycle, this process would fall into Business Understanding phase where we decide the shape of the data set but as explained earlier in the introduction, we will not designate a separate document for the Business Understanding phase. As such, we will integrate our data sources and pre-process procedures into our DQR. 1.2 Data Source See Appendix B for more information on the directory tree. The raw data files are originally placed in data/raw/ from which we just take the relevant data and convert them into CSV format. However, as we found out, the actual data source for the election data used by the New York Times is from Associated Press (AP), whose term of use prohibits distribution of this data set. As such, we must stress that we do not intend to redistribute nor to commercialize the data set but merely use it for research purpose only. However, since this is a data-mining project, we have to show steps we took to transform the data at each stage, it is then unavoidable that we have the election data lurking in our project. As such, we hope we have stated our point clearly that we only use this data set for research purpose. To view the raw data sets, please refer to the links included in the source summaries. For more details, please refer the disclaimer section. Another point to note is that all data sets used have data by counties except for electoral vote data. Following is the description of different data sets used in this study: Table 1.1: Data Source File Description Source data_main The main data source that we are using in this study, consisting of 161 features, covering many domains: election, administration, geographic, demographics, healthcare, human-development, weather, etc. Refer to meta_main.csv for more information. Emil O. W. Kirkegaard geog_10 2010 geographic data where we extract data on area (land) to calculate population density Census Fact-finder population_15 2015 population data where we extract data on total population and voting age population Census Fact-finder age_gender_15 2015 age and gender data where we extract sex ratio, and different age related metrics Census Fact-finder health_14 2014 healthcare data where we extract some health-related data and several socio-economical metrics such as unemployment and crime rate, etc. County Health Ranking health_15 2015 healthcare data where we extract some health-related data and several socio-economical metrics such as unemployment and crime rate, etc. County Health Ranking health_16 2016 healthcare data where we extract some health-related data and several socio-economical metrics such as unemployment and crime rate, etc. County Health Ranking human_dev_09-10 2009-2010 human development data, most of which we take in order to update the date in the data_main table Measure of America life_exp_98-10 1998-2010 data on life expectancy Global Health Data Exchange lectoral_votes_00 2000-present data on electoral votes by states Psephos Adam Carr’s Election Archive As mentioned, the main data set itself is compiled from various sources. However, many of the data are not up-to-date and since our focus is to study the most recent election result, we need to update our data. As such, we trace back the sources from which the compiler of the main data set extracted data and find the most recent versions of these data sets. As for the sources used in the main data set, they are listed below. Table 1.2: Main Dataset Source Dataset Source New York Times 2008 US Election result view New York Times 2012 US Election result view New York Times 2016 US Election result view Measure of America’s county data covering 2009-2010 view Countyhealthrankings.org’s health data for 2014 view Factfinder’s geographic dataset for counties for 2010 view Factfinder’s age and gender dataset for counties for 2010 view NOAA 1980-2010 normals (climate data). Average annual and seasonal data for the last 30 years view Also, there are some point in the approach of the original compiler that we do not quite agree with, for instance with the 2009-2010 Human Development data set, the author Emil O. W. Kirkegaard took the average values of 2009 and 2010, which to us is not a good move because, for county such as District of Columbia (FIPS 11001), the 2010 data are not available, resulting in wrong figures: the median earning drops to $20,254 while it should actually be $40,510). This potentially cause outliers in our data and thus, we will, for now, just update this data to 2010 result and will proceed to our data preparation step to deal with missing values. As for now, we will proceed to tidying up the main data set. 1.3 Data Pre-Process Most of the data are listed by county, so we will merge these data sets into the main data set using county FIPS code. Despite having an impressive set of 161 features, the main data set has many unnecessary features as well as lacks many important ones. As such, we will cover basic processes, including trimming, editing, adding and updating features to the main data set. Afterwards, we re-organize the order of the columns to improve readability and to make sure that features of the same domain/category are close to one another. We give each procedure a different code, for example, the first trimming procedure is called TR001. The purpose is to make it easier for code reference as a huge underlying bulk of this study lies in the code. See Appendix C to view the procedure used and the corresponding code. 1.3.1 Trimming Some of the features in the main data set are calculated directly by the author to serve his research paper, such as S factor, CFS, ACFS, etc. Also, in the precious election data which is scrapped off from the New York Times, there are many features that are irrelevant to our goal, such as the vote counts and fractions for minor parties (independent, other, constitution, etc.); in fact, we have checked through all counties to make sure that no 3rd party won any county. Therefore, these features will be trimmed off. The detail trimming steps and reason for each are documented below. TR001 [COLUMN] green16_frac, libert16_frac, other16_frac [REASON] we will not consider these minority parties. Gary Johnson and Jill Stein did not win a single county so third parties will not affect largely the result between Democratic and Republican party. Nonetheless, we will create a other16_frac column just in case TR002 [COLUMN] all with suffix _frac2 [REASON] they are calculated by the author to just consider 2 major parties in the ratio, but since we want to group all other parties as one, we do not need this TR003 [COLUMN] all with prefix votes16_ except for votes16_trumpd and votes16_clintonh [REASON] we will sum the votes of all third parties to votes16_other TR004 [COLUMN] name_prev, ST, County, State, votes, reporting, precincts, X, Y, X1 [REASON] they are of no use to our usage and merely left there in the dataset as tracers of the election scrapping algorithm that the author used, or they are merely duplicated of some other columns TR005 [COLUMN] At.Least.High.School.Diploma [REASON] not surprisingly this and Less.Than.High.School sum to 100% and so we can count this as redundant TR006 [COLUMN] nearest county, temp, precip, temp_bins, lat_bins, lon_bins, precip_bins, elevation_bins, lon, lat, elevation [REASON] trace or temporary features of the scraping algorithm used to mine weather data TR007 [COLUMN] all with suffix _TMAX, _TMIN, _TAVG, _PRCP, except for winter_PRCP and winter_TAVG [REASON] we do not need all weather data, we will only keep winter data for the period of time during the election as this potentially affect the outcome of the election (e.g. due to bad weather, people cannot vote, etc.) TR008 [COLUMN] White, Black, Hispaic, Asian, Amerindian, Other, White_Asian [REASON] duplicated to the other race features, probably used by the author for his socioeconomic factor calculations TR009 [COLUMN] CA, S, MAR, CFS, ACFS, MeanALC, MaxALC, Mixedness [REASON] those are columns that the author computed in his paper, there are little knowledge of how he computed them as well as how to use them but they are calculated but on the demographics informations that we already have, so we can safely discard them TR010 [ROW] Alaska County (2000) and Oglala Lakota county (46102) and all rows where the county has no name, or no county FIPS code [REASON] Alaska does not have the county system (they have boroughs) and thus we do not have their other demographic data based on county. Oglala Lakota is the Indian reservation area and we do not have most of their demographic data as well. As we use county FIPS code to match data while merging dataset, rows without county FIPS code should be discarded. Evidently, these rows and the rows for county without names have a lot of missing data and should be discarded 1.3.2 Editing ED001 [COLUMN] winter_PRCP [ACTION] divide by 100 [REASON] the scraping algorithm did not divide the precipitation data by 100. This can be verified by looking into the weather scraping code of the author or by going to the weather data source. ED002 [COLUMN] winter_TAVG [ACTION] divide by 10 [REASON] the scraping algorithm did not divide the precipitation data by 10. This can be verified by looking into the weather scraping code of the author or by going to the weather data source 1.3.3 Adding We add several features that we find typically-included in study involving demographic such as: life expectancy, sex ratio, etc. AD001 [COLUMN] other16_frac, votes16_others [REASON] these will represent the number of vote and the vote fraction of other parties in the 2016 election AD002 [COLUMN] elec_rep16_win, elec_rep16_win, elec_rep16_win [REASON] this column indicates if Republican party wins the county in various election year. Essentially, we are considering converting this problem into a two-class problem as we realize that other parties did not win any county AD003 [COLUMN] sex_ratio, age_dependency_ratio, median_age [REASON] some useful and typically-included demographic indicator concerning age and gender (in this step, we also update the median age data) AD004 [COLUMN] life_expectancy [REASON] usuful healthcare and standard of living indicator AD005 [COLUMN] Total.Population, voting_age_population [REASON] we update the total population and add in information regarding voting age population in order to calculate voting participation rate AD006 [COLUMN] voting_power [REASON] each state owns different number of electoral votes which affect the result directly so it is intuitive for candicdate to focus his/her campaign and funding on certain area. This data cannot be computed from the 2016 results so we used the 2012 results instead AD007 [COLUMN] voting_participation [REASON] this also cannot be computed from the 2016 result but by the 2012 result AD008 [COLUMN] population_density [REASON] candidate might be interested in populous region to campaign 1.3.4 Updating UP001 [COLUMN] Poor.physical.health.days, Poor.mental.health.days, Low.birthweight, Teen.births, Children.in.single.parent.households, Adult.smoking, Adult.obesity, Diabetes, Sexually.transmitted.infections, HIV.prevalence.rate, Uninsured, Unemployment, Violent.crime, Homicide.rate, Injury.deaths, Infant.mortality [ACTION] update using 2016 dataset [REASON] update healthcare related data UP002 [COLUMN] Median.Earnings.2010.dollars, Less.Than.High.School, At.Least.Bachelor.s.Degree, Graduate.Degree, School.Enrollment, White.not.Latino.Population, African.American.Population, Native.American.Population, Asian.American.Population, Population.some.other.race.or.races, Latino.Population, Children.Under.6.Living.in.Poverty, Adults.65.and.Older.Living.in.Poverty, Preschool.Enrollment.Ratio.enrolled.ages.3.and.4, Poverty.Rate.below.federal.poverty.threshold, Gini.Coefficient, Child.Poverty.living.in.families.below.the.poverty.line, Management.professional.and.related.occupations, Service.occupations, Sales.and.office.occupations, Farming.fishing.and.forestry.occupations, Construction.extraction.maintenance.and.repair.occupations, Production.transportation.and.material.moving.occupations [ACTION] update using 2010 dataset [REASON] update various human-development related data. This is essentially to undo the compilation step of the original author when he averages 2009 and 2010 data Now that we have most of the redundant and irrelevant features trimmed off from the data as well as we have needed features added, we export our working data set to data/processed/data.csv and its metadata to data/processed/meta.csv. For the reason of potential matching of data in further steps (i.e. handling missing data and outliers in data preparation phase), we will leave the county name as well as their county FIPS code. We also leave the estimating number of vote data to identify what we consider county with unreliable results, in other words, county with the number of reported vote that is not enough to declare a winner. This dataset still contains a lot of features so we feel that the best way to present them is not necessarily list them out in a table, but try to visualize them. As such, we present the following choropleth map. Certainly, this is not the final data set (please note that we will keep a copy of the data at each stage of the preparation process) but we can proceed with some data exploration. See Appendix A to view the main datasets at different stages. 1.4 Data Exploration In this section, we will explore the pre-processed data set. We will present summary of the data as well as explore the correlation between each feature. Talking about quality of the data set, we cannot go without mentioning the distribution of each feature; however, this will be covered in later sections when we go into details of each feature. As for now, we start with some general information of the data set. Table 1.3: General Information Number of instances 3111 Number of features 79 Number of continuous features 73 Number of binary features 3 Number of nominal features 3 Number of missing values 6985 Percentage of missing values 3 Maximum Entropy for a feature 12 In term of the structure of the data, since we work with R, we will present structure of the data in R style. This is a long table (79 columns), to view the rest, please use the pagination controller at the bottom-right corner The data set consists of both continuous and nominal features and each type has different aspects to explore. As such, we present 2 separate tables for summary of each type of features. For continuous features, general statistical values are presented, including mean, median, standard deviation, min and max, as well as interquartile range, upper quartile (75%) and lower quartile (25%), percentage and number of missing values (NA). Shannon entropy is included in here as we thought it might be useful to detect extreme data, but perhaps it is more useful for nominal features. These tables are wide, scroll right to see the rest of the summary For categorical/nominal features, we also include statistics on the mode, highest frequency and a peek at different levels of the data. This table also summarizes binary features that we derived earlier in the pre-processing phase, at this point, we are already aware of the fact that the number of counties that Republican won in presidential elections are usually pretty high, although it seems like Democrat got hold of many populous and important counties–hence the overall in the 2008 and 2012 election. Nevertheless, as we proceed further, especially during modelling phase, we need to apply some sampling technique to balance out this distribution of class value for the winner. Traditionally, we need to include a scatter plot matrix for all the feature and study their correlation, but we might spare that this time as we have quite a formidable number of numeric features (73), not including the FIPS codes, which will result in a giant table. To satisfy our curiosity, we actually did try to construct such a plot and apparently that was not so aesthetically pleasing. The lower triangle shows the pairwise scatter plots between every features, the upper triangle shows the corresponding Pearson correlation value and the p-value (the larger the number, the bigger the font used). Last but not least, we designated the diagonal for histogram plot of each feature. As mentioned the overall visual presentation is overwhelming but not so effective. We are especially interested in the distribution of the data and thus, we will do justice to histogram in the later section where we talk about each feature in detail. Click here to download the high resolution version. Figure 1.1: Scatter Plot Matrix Our second attempt at correlation plot (using the corrplot package in R) is more visually effective. Arguably, this is even better than looking at a table of correlation values between all pairs of features because when we pre-process the data set, we reordered them into categories so now it is really easy for us to spot highly correlated groups of features, which potentially provide us with some insight on how to reduce the dimension of the data set. The correlation table will be huge though (~2500 rows) so we will only take a peek at the top 100 correlation. Fear not the minuscule! Hover on the the plot to zoom Figure 1.2: Correlation Plot Matrix From the correlation matrix plot, we can identify a few groups that are highly correlated. First, at the top-left, we can see that election results between the year and even between parties are highly correlated. This might seems bizarre at first, but that actually makes a lot of sense. First, 2008 and 2012 results are very similar as Barrack Obama (Democratic party) won both round. Of course, the vote fraction features are not highly correlated to the vote count, but they are highly correlated to one another. The situation seems even more absurd when we have vote count for Donald Trump and Hillary closely related! All of this happens because of the fact that the votes distribution are always very close to the 50:50 line at each county, even if one party leads, it only leads by 60%-40% typically. As such, it comes at no surprise to us that features in this group are highly correlated. The implication of this is huge because this group contains the target that we are to select for our supervised modelling process. We will reserve that for section @(dqp). Other highly-correlated groups are the poverty/finance group and the healthcare group. Again, it is not an astonishing result. Although these two groups are highly correlated, there are (sporadically) some of their features that are not pairwise strongly correlated. This means that we cannot just easily get rid of them. As we will see in data preparation phase, this group of feature will also be reduced but in a much different way as compared to the group (election result) mentioned previously. 1.5 Feature Detail In this final major section of this report, we will present the detail of each feature. These details include the meta data, formula, etc. and most importantly a histogram to show the distribution of value of each feature. As we can see, most will be right-skewed. We, however, cannot determine the exact underlying function for these skewed distribution and thus cannot do anything about that. For learners that are heavily affected by this, we can discretize them to secure performance. Following this section will be section 1.6, Deleted Feature, which gives details of features that we decide to remove from the final data set (Analytic Base Table) after data preparation and feature selection phase. FIPS State Code Original name: statecode_prev Variable name: fips_state Display name: FIPS State Code Category: administration Unit: NA Meaning: FIPS previous state code Lowerbound: NA Upperbound: NA Remark: used for matching data between the NYT datasets Year: 2010 Source: http://factfinder.census.gov/bkmk/table/1.0/en/DEC/10_SF1/G001/0100000US.05000.003 Republican Win 2016 Original name: rep16_win Variable name: elec_rep16_win Display name: Republican Win 2016 Category: election Unit: NA Meaning: indicate whether Republican party win the county in 2016 election or not Lowerbound: NA Upperbound: NA Remark: Year: 2016 Source: http://www.nytimes.com/elections/results/president Republican Win 2012 Original name: rep12_win Variable name: elec_rep12_win Display name: Republican Win 2012 Category: election Unit: NA Meaning: indicate whether Republican party win the county in 2012 election or not Lowerbound: NA Upperbound: NA Remark: Year: 2012 Source: http://www.nytimes.com/elections/2012/results/president.html Voting Participation Rate Original name: voting_participation Variable name: elec_voting_participation Display name: Voting Participation Rate Category: election Unit: % Meaning: percentage of population participated in the voting Lowerbound: 0 Upperbound: 100 Remark: cannot use 2016 results to calculate this because we are trying to predict the result Year: 2012 Source: http://www.nytimes.com/elections/2012/results/president.html \\[= \\frac{\\text{total number of votes in 2012}}{\\text{total voting-population 2015}}\\times{100} \\] Voting Power Original name: voting_power Variable name: elec_voting_power Display name: Voting Power Category: election Unit: NA Meaning: the power of each vote that counts towards the final vote of each state Lowerbound: 0 Upperbound: NA Remark: Year: 2000-2016 Source: http://psephos.adam-carr.net/countries/u/usa/pres/2000.txt \\[= \\frac{\\text{state electoral votes } \\times \\text{ total number of votes of the county}}{\\text{total number of votes of the state}} \\] Population Density Original name: population_density Variable name: demo_population_density Display name: Population Density Category: demographics Unit: per km^2 Meaning: population density Lowerbound: 0 Upperbound: NA Remark: Year: 2015 Source: http://factfinder.census.gov/bkmk/table/1.0/en/DEC/10_SF1/G001/0100000US.05000.003 \\[= \\frac{\\text{total population}}{\\text{total land area}} \\] Sex Ratio Original name: sex_ratio Variable name: demo_sex_ratio Display name: Sex Ratio Category: demographics Unit: NA Meaning: number of males per 100 females Lowerbound: 0 Upperbound: NA Remark: Year: 2015 Source: https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?pid=ACS_15_5YR_S0101&amp;prodType=table \\[= \\frac{\\text{number of males in the population}}{\\text{number of females in the population}}\\times{100} \\] Age Dependency Ratio Original name: age_dependency_ratio Variable name: demo_age_dependency_ratio Display name: Age Dependency Ratio Category: demographics Unit: % Meaning: number of dependents, aged zero to 14 and over the age of 65, to the total population, aged 15 to 64 Lowerbound: 0 Upperbound: NA Remark: Year: 2015 Source: https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?pid=ACS_15_5YR_S0101&amp;prodType=table \\[= \\frac{\\text{number of people not in working age}}{\\text{number of people in working age}}\\times{100} \\] Median Age Original name: median_age Variable name: demo_median_age Display name: Median Age Category: demographics Unit: year Meaning: median age Lowerbound: 0 Upperbound: NA Remark: Year: 2015 Source: https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?pid=ACS_15_5YR_S0101&amp;prodType=table White not Latino Population Original name: White.not.Latino.Population Variable name: population_white Display name: White not Latino Population Category: race Unit: % Meaning: percentage of populaton identified themselves as white not latino Lowerbound: 0 Upperbound: 100 Remark: Year: 2010 Source: http://www.measureofamerica.org/download-agreement/ \\[= \\frac{\\text{number of people who idenfitied themselves as white (non-latino)}}{\\text{total population}}\\times{100} \\] African American Population Original name: African.American.Population Variable name: population_african_american Display name: African American Population Category: race Unit: % Meaning: percentage of populaton identified themselves as african american Lowerbound: 0 Upperbound: 100 Remark: Year: 2010 Source: http://www.measureofamerica.org/download-agreement/ \\[= \\frac{\\text{number of people who idenfitied themselves as african american }}{\\text{total population}}\\times{100} \\] Native American Population Original name: Native.American.Population Variable name: population_native Display name: Native American Population Category: race Unit: % Meaning: percentage of populaton identified themselves as native american Lowerbound: 0 Upperbound: 100 Remark: Year: 2010 Source: http://www.measureofamerica.org/download-agreement/ \\[= \\frac{\\text{number of people who idenfitied themselves as native american }}{\\text{total population}}\\times{100} \\] Asian American Population Original name: Asian.American.Population Variable name: population_asian Display name: Asian American Population Category: race Unit: % Meaning: percentage of populaton identified themselves as asian american Lowerbound: 0 Upperbound: 100 Remark: Year: 2010 Source: http://www.measureofamerica.org/download-agreement/ \\[= \\frac{\\text{number of people who idenfitied themselves as asian american }}{\\text{total population}}\\times{100} \\] Other race(s) Population Original name: Population.some.other.race.or.races Variable name: population_other Display name: Other race(s) Population Category: race Unit: % Meaning: percentage of populaton identified themselves as other race Lowerbound: 0 Upperbound: 100 Remark: Year: 2010 Source: http://www.measureofamerica.org/download-agreement/ \\[= \\frac{\\text{number of people who idenfitied themselves as other races}}{\\text{total population}}\\times{100} \\] Latino Population Original name: Latino.Population Variable name: population_latino Display name: Latino Population Category: race Unit: % Meaning: percentage of populaton identified themselves as latino/hispanic Lowerbound: 0 Upperbound: 100 Remark: Year: 2010 Source: http://www.measureofamerica.org/download-agreement/ \\[= \\frac{\\text{number of people who idenfitied themselves as latino/hispanic }}{\\text{total population}}\\times{100} \\] Homogeneity Index Original name: SIRE_homogeneity Variable name: population_homnogenity_index Display name: Homogeneity Index Category: race Unit: NA Meaning: Simpson index (or 1 - Simpson diversity index) Lowerbound: 0 Upperbound: 1 Remark: Year: 2010 Source: Kirkegaard \\[= 1 - \\frac{\\sum{n(n-1)}}{N(N-1)} \\] Gini Coefficient Original name: Gini.Coefficient Variable name: finance_gini_coef Display name: Gini Coefficient Category: finance Unit: NA Meaning: income inequality index Lowerbound: 0 Upperbound: 1 Remark: Year: 2010 Source: http://www.measureofamerica.org/download-agreement/ \\[= \\text{formula depends on distribution of income} \\] Median Earning Original name: Median.Earnings.2010.dollars Variable name: finance_median_earning Display name: Median Earning Category: finance Unit: dollars Meaning: median earning of individuals Lowerbound: 0 Upperbound: NA Remark: Year: 2010 Source: http://www.measureofamerica.org/download-agreement/ Unemployment Rate Original name: Unemployment Variable name: demo_unemployment Display name: Unemployment Rate Category: occupation Unit: % Meaning: percentage of population age &gt;= 16 unemployed and/or looking for job Lowerbound: 0 Upperbound: 100 Remark: Year: 2016 Source: http://www.countyhealthrankings.org/rankings/data \\[= \\frac{\\text{number of people age} \\geq 16 \\text{ unemployed and/or looking for job}}{\\text{total population}}\\times{100} \\] Management, Professional, and Related Occupations Original name: Management.professional.and.related.occupations Variable name: occupation_management Display name: Management, Professional, and Related Occupations Category: occupation Unit: % Meaning: percentage of population having management, professional, and related occupations Lowerbound: 0 Upperbound: 100 Remark: Year: 2010 Source: http://www.measureofamerica.org/download-agreement/ \\[= \\frac{\\text{number of people having management, professional, and related occupations}}{\\text{total population}}\\times{100} \\] Service Occupations Original name: Service.occupations Variable name: occupation_service Display name: Service Occupations Category: occupation Unit: % Meaning: percentage of population having service occupations Lowerbound: 0 Upperbound: 100 Remark: Year: 2010 Source: http://www.measureofamerica.org/download-agreement/ \\[= \\frac{\\text{number of people having service occupations}}{\\text{total population}}\\times{100} \\] Sales and Office Occupations Original name: Sales.and.office.occupations Variable name: occupation_sale Display name: Sales and Office Occupations Category: occupation Unit: % Meaning: percentage of population having sales and office occupations Lowerbound: 0 Upperbound: 100 Remark: Year: 2010 Source: http://www.measureofamerica.org/download-agreement/ \\[= \\frac{\\text{number of people having sales and office occupations}}{\\text{total population}}\\times{100} \\] Farming, Fishing, and Forestry Occupations Original name: Farming.fishing.and.forestry.occupations Variable name: occupation_agriculture Display name: Farming, Fishing, and Forestry Occupations Category: occupation Unit: % Meaning: percentage of population having agricultural occupations Lowerbound: 0 Upperbound: 100 Remark: Year: 2010 Source: http://www.measureofamerica.org/download-agreement/ \\[= \\frac{\\text{number of people having agricultural occupations}}{\\text{total population}}\\times{100} \\] Construction, Extraction, Maintenance, and Repair Occupations Original name: Construction.extraction.maintenance.and.repair.occupations Variable name: occupcation_maintainance Display name: Construction, Extraction, Maintenance, and Repair Occupations Category: occupation Unit: % Meaning: percentage of population having construction, extraction, maintenance, and repair occupations Lowerbound: 0 Upperbound: 100 Remark: Year: 2010 Source: http://www.measureofamerica.org/download-agreement/ \\[= \\frac{\\text{number of people having construction, extraction, maintenance, and repair occupations}}{\\text{total population}}\\times{100} \\] Production, Transportation, and Material moving Occupations Original name: Production.transportation.and.material.moving.occupations Variable name: occupation_transportation Display name: Production, Transportation, and Material moving Occupations Category: occupation Unit: % Meaning: percentage of population having production, transportation, and material moving occupations Lowerbound: 0 Upperbound: 100 Remark: Year: 2010 Source: http://www.measureofamerica.org/download-agreement/ \\[= \\frac{\\text{number of people having production, transportation, and material moving occupations}}{\\text{total population}}\\times{100} \\] School Enrollment Original name: School.Enrollment Variable name: education_school_enrollment Display name: School Enrollment Category: education Unit: % Meaning: percentage of school enrollment Lowerbound: 0 Upperbound: 100 Remark: Year: 2010 Source: http://www.measureofamerica.org/download-agreement/ \\[= \\frac{\\text{number of actual students enrolled}}{\\text{number of potential students enrolled}}\\times{100} \\] Preschool Enrollment Ratio Original name: Preschool.Enrollment.Ratio.enrolled.ages.3.and.4 Variable name: education_preschool_enrollment Display name: Preschool Enrollment Ratio Category: education Unit: % Meaning: percentage of preschool enrollment (between 3 and 4 years old) Lowerbound: 0 Upperbound: 100 Remark: Year: 2010 Source: http://www.measureofamerica.org/download-agreement/ \\[= \\frac{\\text{number of actual children (between 3 and 4 years old) enrolled in preschool}}{\\text{number of potential children enrolled in preschool}}\\times{100} \\] At Least Bachelor Degree Original name: At.Least.Bachelor.s.Degree Variable name: education_at_least_bachelor Display name: At Least Bachelor Degree Category: education Unit: % Meaning: percentage of population with at least bachelor degree Lowerbound: 0 Upperbound: 100 Remark: Year: 2010 Source: http://www.measureofamerica.org/download-agreement/ \\[= \\frac{\\text{number of people with at least bachelor degree}}{\\text{total population}}\\times{100} \\] Graduate Degree Original name: Graduate.Degree Variable name: education_graduate_degree Display name: Graduate Degree Category: education Unit: % Meaning: percentage of population with graduate degree Lowerbound: 0 Upperbound: 100 Remark: Year: 2010 Source: http://www.measureofamerica.org/download-agreement/ \\[= \\frac{\\text{number of people with at least graduate degree}}{\\text{total population}}\\times{100} \\] Infant Mortality Original name: Infant.mortality Variable name: health_infant_mortality Display name: Infant Mortality Category: healthcare Unit: NA Meaning: number of deaths of infants under 1 year old per 1,000 live births Lowerbound: 0 Upperbound: 1000 Remark: Year: 2016 Source: http://www.countyhealthrankings.org/rankings/data \\[= \\frac{\\text{number of deaths of infants under 1 year old}}{\\text{number of live birth}}\\times{1000} \\] Injury Death Rate Original name: Injury.deaths Variable name: health_injury Display name: Injury Death Rate Category: healthcare Unit: NA Meaning: injury mortality rate per 100,000 Lowerbound: 0 Upperbound: 1e+05 Remark: Year: 2016 Source: http://www.countyhealthrankings.org/rankings/data \\[= \\frac{\\text{number of injury deaths}}{\\text{total population}}\\times{100000} \\] Low Birth Weight Rate Original name: Low.birthweight Variable name: health_low_birth_weight Display name: Low Birth Weight Rate Category: healthcare Unit: % Meaning: percentage of births with low birth weight (&lt;2,500g) Lowerbound: 0 Upperbound: 100 Remark: Year: 2016 Source: http://www.countyhealthrankings.org/rankings/data \\[= \\frac{\\text{number of births with low birth weight(&lt; 2,500 g)}}{\\text{number of births}}\\times{100} \\] Children in Single Parent Households Rate Original name: Children.in.single.parent.households Variable name: health_children_with_single_parent Display name: Children in Single Parent Households Rate Category: healthcare Unit: % Meaning: percentage of children living in single-parent household Lowerbound: 0 Upperbound: 100 Remark: Year: 2016 Source: http://www.countyhealthrankings.org/rankings/data \\[= \\frac{\\text{number of children living in single-parent household}}{\\text{number of children}}\\times{100} \\] Adult Obesity Rate Original name: Adult.obesity Variable name: health_adult_obesity Display name: Adult Obesity Rate Category: healthcare Unit: % Meaning: percentage of adults that report BMI &gt;= 30 Lowerbound: 0 Upperbound: 100 Remark: Year: 2016 Source: http://www.countyhealthrankings.org/rankings/data \\[= \\frac{\\text{number of adults reported BMI} \\geq 30}{\\text{number of adults}}\\times{100} \\] Diabetes Rate Original name: Diabetes Variable name: health_diabetes Display name: Diabetes Rate Category: healthcare Unit: % Meaning: percentage of adults that reported currently having diabetes Lowerbound: 0 Upperbound: 100 Remark: Year: 2016 Source: http://www.countyhealthrankings.org/rankings/data \\[= \\frac{\\text{number of adults currently reported having diabetes}}{\\text{number of adults}}\\times{100} \\] Sexually Transmitted Infections Rate Original name: Sexually.transmitted.infections Variable name: health_sti Display name: Sexually Transmitted Infections Rate Category: healthcare Unit: NA Meaning: number of Chlamydia case in population per 100,000 Lowerbound: 0 Upperbound: 1e+05 Remark: Year: 2016 Source: http://www.countyhealthrankings.org/rankings/data \\[= \\frac{\\text{number of people with Chlamydia}}{\\text{total population}}\\times{100000} \\] HIV Prevalence Rate Original name: HIV.prevalence.rate Variable name: health_hiv Display name: HIV Prevalence Rate Category: healthcare Unit: NA Meaning: number of people living with HIV infection in a given population at a given period of time per 100,000 Lowerbound: 0 Upperbound: 1e+05 Remark: Year: 2016 Source: http://www.countyhealthrankings.org/rankings/data \\[= \\frac{\\text{number of people currently reported living with HIV infection}}{\\text{total population}}\\times{100000} \\] Uninsured Rate Original name: Uninsured Variable name: demo_uninsured Display name: Uninsured Rate Category: healthcare Unit: % Meaning: percentage of population under 65 without insurance Lowerbound: 0 Upperbound: 100 Remark: Year: 2016 Source: http://www.countyhealthrankings.org/rankings/data \\[= \\frac{\\text{number of people under 65 without insurance}}{\\text{total population}}\\times{100} \\] Crime Rate Original name: Violent.crime Variable name: demo_violent_crime Display name: Crime Rate Category: crime Unit: NA Meaning: number of crimes reported to law enforcement agencies per 100,000 total population Lowerbound: 0 Upperbound: 1e+05 Remark: Year: 2016 Source: http://www.countyhealthrankings.org/rankings/data \\[= \\frac{\\text{number of violent crimes}}{\\text{total population}}\\times{100000} \\] Homicide Rate Original name: Homicide.rate Variable name: demo_homicide Display name: Homicide Rate Category: crime Unit: NA Meaning: homicide mortality rate per 100,000 total population Lowerbound: 0 Upperbound: 1e+05 Remark: Year: 2016 Source: http://www.countyhealthrankings.org/rankings/data \\[= \\frac{\\text{number of homicide crimes}}{\\text{total population}}\\times{100000} \\] Winter Avg. Precipitation Original name: winter_PRCP Variable name: weather_winter_prcp Display name: Winter Avg. Precipitation Category: weather Unit: inch Meaning: winter precipitation Lowerbound: 0 Upperbound: NA Remark: during the election time Year: 1981-2010 Source: https://www.ncdc.noaa.gov/cdo-web/datatools/normals Winter Avg. Temperature Original name: winter_TAVG Variable name: weather_winter_tavg Display name: Winter Avg. Temperature Category: weather Unit: F Meaning: winter average temperature Lowerbound: -200 Upperbound: 200 Remark: during the election time Year: 1981-2010 Source: https://www.ncdc.noaa.gov/cdo-web/datatools/normals 1.6 Deleted Feature FIPS County Code Original name: fips Variable name: fips_county Display name: FIPS County Code Category: administration Unit: NA Meaning: FIPS county code Lowerbound: 0 Upperbound: 57000 Remark: FIPS is federal Information Processing Standard Year: 2016 Source: http://www.nytimes.com/elections/results/president \\[= \\text{2 digits state FIPS code} + \\text{(up to) 3 digits for county code} \\] County Name Original name: name_16 Variable name: name Display name: County Name Category: administration Unit: NA Meaning: county name Lowerbound: NA Upperbound: NA Remark: Year: 2016 Source: http://www.nytimes.com/elections/results/president Republican Win 2008 Original name: rep08_win Variable name: elec_rep08_win Display name: Republican Win 2008 Category: election Unit: NA Meaning: indicate whether Republican party win the county in 2008 election or not Lowerbound: NA Upperbound: NA Remark: Year: 2008 Source: http://www.nytimes.com/elections/2008/results/president/map.html Republican Votes 2016 Original name: votes16_trumpd Variable name: elec_rep16 Display name: Republican Votes 2016 Category: election Unit: NA Meaning: number of votes of Donald J. Trump Lowerbound: 0 Upperbound: NA Remark: republican party Year: 2016 Source: http://www.nytimes.com/elections/results/president Republican Votes 2012 Original name: rep12 Variable name: elec_rep12 Display name: Republican Votes 2012 Category: election Unit: NA Meaning: number of votes for republican party 2012 Lowerbound: 0 Upperbound: NA Remark: Year: 2012 Source: http://www.nytimes.com/elections/2012/results/president.html Republican Votes 2008 Original name: rep08 Variable name: elec_rep08 Display name: Republican Votes 2008 Category: election Unit: NA Meaning: number of votes for republican party 2008 Lowerbound: 0 Upperbound: NA Remark: Year: 2008 Source: http://www.nytimes.com/elections/2008/results/president/map.html Democratic Votes 2016 Original name: votes16_clintonh Variable name: elec_dem16 Display name: Democratic Votes 2016 Category: election Unit: NA Meaning: number of votes of Hillary Clinton Lowerbound: 0 Upperbound: NA Remark: democratic party Year: 2016 Source: http://www.nytimes.com/elections/results/president Democratic Votes 2012 Original name: dem12 Variable name: elec_dem12 Display name: Democratic Votes 2012 Category: election Unit: NA Meaning: number of votes for democratic party 2012 Lowerbound: 0 Upperbound: NA Remark: Year: 2012 Source: http://www.nytimes.com/elections/2012/results/president.html Democratic Votes 2008 Original name: dem08 Variable name: elec_dem08 Display name: Democratic Votes 2008 Category: election Unit: NA Meaning: number of votes for democratic party 2008 Lowerbound: 0 Upperbound: NA Remark: Year: 2008 Source: http://www.nytimes.com/elections/2008/results/president/map.html Other Parties Votes 2016 Original name: votes16_others Variable name: elec_other16 Display name: Other Parties Votes 2016 Category: election Unit: NA Meaning: number of votes of other parties than Democratic and Republic 2016 Lowerbound: 0 Upperbound: NA Remark: Year: 2016 Source: http://www.nytimes.com/elections/results/president Other Votes 2012 Original name: other12 Variable name: elec_other12 Display name: Other Votes 2012 Category: election Unit: NA Meaning: number of votes for other party 2012 Lowerbound: 0 Upperbound: NA Remark: Year: 2012 Source: http://www.nytimes.com/elections/2012/results/president.html Other Votes 2008 Original name: other08 Variable name: elec_other08 Display name: Other Votes 2008 Category: election Unit: NA Meaning: number of votes for other party 2008 Lowerbound: 0 Upperbound: NA Remark: Year: 2008 Source: http://www.nytimes.com/elections/2008/results/president/map.html Total Votes 2016 Original name: total16 Variable name: elec_total16 Display name: Total Votes 2016 Category: election Unit: NA Meaning: total number of votes 2016 Lowerbound: 0 Upperbound: NA Remark: Year: 2016 Source: http://www.nytimes.com/elections/results/president Total Votes 2012 Original name: total12 Variable name: elec_total12 Display name: Total Votes 2012 Category: election Unit: NA Meaning: total number of votes 2012 Lowerbound: 0 Upperbound: NA Remark: Year: 2012 Source: http://www.nytimes.com/elections/2012/results/president.html Total Votes 2008 Original name: total08 Variable name: elec_total08 Display name: Total Votes 2008 Category: election Unit: NA Meaning: total number of votes 2008 Lowerbound: 0 Upperbound: NA Remark: Year: 2008 Source: http://www.nytimes.com/elections/2008/results/president/map.html Republican Vote Fraction 2016 Original name: rep16_frac Variable name: elec_rep16_frac Display name: Republican Vote Fraction 2016 Category: election Unit: NA Meaning: vote fraction of republican 2016 Lowerbound: 0 Upperbound: 1 Remark: Year: 2010 Source: Kirkegaard Republican Vote Fraction 2012 Original name: rep12_frac Variable name: elec_rep12_frac Display name: Republican Vote Fraction 2012 Category: election Unit: NA Meaning: vote fraction of republican 2012 Lowerbound: 0 Upperbound: 1 Remark: Year: 2010 Source: Kirkegaard Republican Vote Fraction 2008 Original name: rep08_frac Variable name: elec_rep08_frac Display name: Republican Vote Fraction 2008 Category: election Unit: NA Meaning: vote fraction of republican 2008 Lowerbound: 0 Upperbound: 1 Remark: Year: 2010 Source: Kirkegaard Democratic Vote Fraction 2016 Original name: dem16_frac Variable name: elec_dem16_frac Display name: Democratic Vote Fraction 2016 Category: election Unit: NA Meaning: vote fraction of democratic 2016 Lowerbound: 0 Upperbound: 1 Remark: Year: 2010 Source: Kirkegaard Democratic Vote Fraction 2012 Original name: dem12_frac Variable name: elec_dem12_frac Display name: Democratic Vote Fraction 2012 Category: election Unit: NA Meaning: vote fraction of democratic 2012 Lowerbound: 0 Upperbound: 1 Remark: Year: 2010 Source: Kirkegaard Democratic Vote Fraction 2008 Original name: dem08_frac Variable name: elec_dem08_frac Display name: Democratic Vote Fraction 2008 Category: election Unit: NA Meaning: vote fraction of democratic 2008 Lowerbound: 0 Upperbound: 1 Remark: Year: 2010 Source: Kirkegaard Other Parties Vote Fraction 2016 Original name: other16_frac Variable name: elec_other16_frac Display name: Other Parties Vote Fraction 2016 Category: election Unit: NA Meaning: vote fraction of other parties than Democratic and Republic 2016 Lowerbound: 0 Upperbound: 1 Remark: Year: 2016 Source: http://www.nytimes.com/elections/results/president Other Vote Fraction 2012 Original name: other12_frac Variable name: elec_other12_frac Display name: Other Vote Fraction 2012 Category: election Unit: NA Meaning: vote fraction of other 2012 Lowerbound: 0 Upperbound: 1 Remark: Year: 2010 Source: Kirkegaard Other Vote Fraction 2008 Original name: other08_frac Variable name: elec_other08_frac Display name: Other Vote Fraction 2008 Category: election Unit: NA Meaning: vote fraction of other 2008 Lowerbound: 0 Upperbound: 1 Remark: Year: 2010 Source: Kirkegaard Estimated No. of Remaining Votes Original name: est_votes_remaining Variable name: elec_est_votes_remaining Display name: Estimated No. of Remaining Votes Category: election Unit: NA Meaning: number of votes from unreported precincts in the county Lowerbound: 0 Upperbound: NA Remark: can be used to check which county has unreliable result Year: 2016 Source: http://www.nytimes.com/elections/results/president Voting Age Population Original name: voting_age_population Variable name: demo_voting_age_population Display name: Voting Age Population Category: demographics Unit: NA Meaning: number of people who are eligible to vote Lowerbound: 0 Upperbound: NA Remark: can be used to compute voting participation rate Year: 2015 Source: https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?pid=ACS_15_5YR_DP05&amp;prodType=table Population Original name: Total.Population Variable name: demo_population Display name: Population Category: demographics Unit: NA Meaning: total population Lowerbound: 0 Upperbound: NA Remark: Year: 2015 Source: https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?pid=ACS_15_5YR_DP05&amp;prodType=table Children Under 6 Living in Poverty Original name: Children.Under.6.Living.in.Poverty Variable name: poverty_children Display name: Children Under 6 Living in Poverty Category: finance Unit: % Meaning: percentage of children under 6 living in poverty Lowerbound: 0 Upperbound: 100 Remark: Year: 2010 Source: http://www.measureofamerica.org/download-agreement/ \\[= \\frac{\\text{number of children under 6 living in povery}}{\\text{number of children under 6}}\\times{100} \\] Child Poverty Original name: Child.Poverty.living.in.families.below.the.poverty.line Variable name: poverty_child_living_in_families Display name: Child Poverty Category: finance Unit: % Meaning: percentage of children living in families below poverty line Lowerbound: 0 Upperbound: 100 Remark: Year: 2010 Source: http://www.measureofamerica.org/download-agreement/ \\[= \\frac{\\text{number of children living in families below poverty line}}{\\text{number of children}}\\times{100} \\] Adults 65 and Older Living in Poverty Original name: Adults.65.and.Older.Living.in.Poverty Variable name: poverty_adult Display name: Adults 65 and Older Living in Poverty Category: finance Unit: % Meaning: percentage of adults 65 and older living in poverty Lowerbound: 0 Upperbound: 100 Remark: Year: 2010 Source: http://www.measureofamerica.org/download-agreement/ \\[= \\frac{\\text{number of adults 65 and older living in povery}}{\\text{number of adults 65 and older}}\\times{100} \\] Poverty Rate Original name: Poverty.Rate.below.federal.poverty.threshold Variable name: poverty_below_federal_threshold Display name: Poverty Rate Category: finance Unit: % Meaning: poverty rate below federal poverty threshold Lowerbound: 0 Upperbound: 100 Remark: Year: 2010 Source: http://www.measureofamerica.org/download-agreement/ \\[= \\frac{\\text{number of people with income below federal poverty threshold}}{\\text{total population}}\\times{100} \\] Less Than High School Original name: Less.Than.High.School Variable name: education_less_than_high_school Display name: Less Than High School Category: education Unit: % Meaning: percentage of population with education level less than highschool Lowerbound: 0 Upperbound: 100 Remark: Year: 2010 Source: http://www.measureofamerica.org/download-agreement/ \\[= \\frac{\\text{number of people with education level less than highschool}}{\\text{total population}}\\times{100} \\] Life Expectancy Original name: life_expectancy Variable name: healthcare_life_expectancy Display name: Life Expectancy Category: healthcare Unit: year Meaning: the average number of years a person born in a given country is expected to live if mortality rates at each age were to remain steady in the future Lowerbound: 0 Upperbound: NA Remark: Year: 2010 Source: http://ghdx.healthdata.org/record/united-states-life-expectancy-estimates-county-1985-2010 Avg. No. of Physically Unhealthy Day Original name: Poor.physical.health.days Variable name: health_poor_physiscal_health_days Display name: Avg. No. of Physically Unhealthy Day Category: healthcare Unit: day Meaning: average number of reported physically unhealthy days per month Lowerbound: 0 Upperbound: NA Remark: Year: 2016 Source: http://www.countyhealthrankings.org/rankings/data Avg. No. of Mentally Unhealthy Day Original name: Poor.mental.health.days Variable name: health_poor_mental_health_days Display name: Avg. No. of Mentally Unhealthy Day Category: healthcare Unit: day Meaning: average number of reported mentally unhealthy days per month Lowerbound: 0 Upperbound: NA Remark: Year: 2016 Source: http://www.countyhealthrankings.org/rankings/data Teen Birth Rate Original name: Teen.births Variable name: health_teen_birth Display name: Teen Birth Rate Category: healthcare Unit: NA Meaning: number of females ages 15-19 pregnant per 1,000 Lowerbound: 0 Upperbound: 1000 Remark: Year: 2016 Source: http://www.countyhealthrankings.org/rankings/data \\[= \\frac{\\text{number of female age 15-19 pregnant}}{\\text{number of female age 15-19}}\\times{1000} \\] Adult Smoking Rate Original name: Adult.smoking Variable name: health_adult_smoking Display name: Adult Smoking Rate Category: healthcare Unit: % Meaning: percentage of adults that reported currently smoking Lowerbound: 0 Upperbound: 100 Remark: Year: 2016 Source: http://www.countyhealthrankings.org/rankings/data \\[= \\frac{\\text{number of adults reported currently smoking}}{\\text{number of adults}}\\times{100} \\] References "],
["dqp.html", "Section 2 Data Quality Plan (DQP) 2.1 Overview 2.2 Handling Outliers 2.3 Feature Trimming 2.4 Handling Missing Data 2.5 Feature Selection 2.6 Feature Space Reduction 2.7 Analytic Base Table (ABT)", " Section 2 Data Quality Plan (DQP) 2.1 Overview This plan gives the detail of the procedure by which we prepare the data: cleansing outliers and handling missing data, as well as trimming and selecting the subset of features (we aim to reduce to 40 features) that we will use for modelling as too many features, even if they are not redundant or irrelevant, renders the model less useful or requires exponentially more data to make sure the learned model reliable, i.e. the Curse of Dimensionality. Below, we give the brief summary of procedures that we will follow alongside with their code labels. After that, we will go into details of each procedure and finally conclude this plan with the analytic base table (ABT) fully-ready for modelling step. In general, the purpose of making the DQR and DQP is for anyone who wants to reproduce this study to base on to construct an ABT that is identical to the one we use for modelling. See Appendix C to view the procedure used and the corresponding code. Handling Outlier HO001 [ROW] Whitman County (53075) [ACTION] trim [REASON] we find the state that has unreliable voting results based on the current number of remaining vote and the difference between the votes of 2 major parties HO002 [ROW] voting_participation with value exceeding upper bound [ACTION] clamp [REASON] we find that some county has higher voting participation rate than 100% which seems impossible but the reason is mainly due to how we derive this feature Feature Trimming TR011 [COLUMN] name_16 [REASON] this feature is absolutely not needed: including it will be as bad as including couty FIPS code TR012 [COLUMN] dem08, rep08, total08, other08, dem12, rep12, total12, other12, votes16_trumpd, votes16_clintonh, total16, rep16_frac, dem16_frac, rep12_frac, rep08_frac, dem12_frac, dem08_frac, other12_frac, other08_frac, votes16_others, other16_frac [REASON] we decided to choose our target as elec_rep16_win and so we can get rid of these features TR013 [COLUMN] rep08_win, Total.Population [REASON] we can get rid of the 2008 result as it is extremely similar to the 2012 result so we can just use the 2012 result alone. As for the population, we find the better indicator in the same domain is population density so we can get rid of this feature TR014 [COLUMN] est_votes_remaining, voting_age_population [REASON] these features are used to find the flipable county and now can be discarded Handling Missing Values HM001 [COLUMN] Low.birthweight, Teen.births, Sexually.transmitted.infections, HIV.prevalence.rate, Violent.crime, Homicide.rate, Injury.deaths, Infant.mortality [REASON] predict the missing values for healthcare data from data of previous years HM002 [COLUMN] Less.Than.High.School, At.Least.Bachelor.s.Degree, Graduate.Degree, School.Enrollment, Adults.65.and.Older.Living.in.Poverty, Preschool.Enrollment.Ratio.enrolled.ages.3.and.4, Median.Earnings.2010.dollars [REASON] predict the missing values for human-development data from data of previous years HM003 [COLUMN] winter_PRCP, winter_TAVG [REASON] predict the missing values based on data of adjacent counties HM004 [COLUMN] winter_PRCP, winter_TAVG, Low.birthweight, Teen.births, Sexually.transmitted.infections, HIV.prevalence.rate, Violent.crime, Homicide.rate, Injury.deaths, Infant.mortality, Adults.65.and.Older.Living.in.Poverty, Preschool.Enrollment.Ratio.enrolled.ages.3.and.4, Median.Earnings.2010.dollars [REASON] predict the missing values based on the average of the data from counties within the same state HM005 [COLUMN] winter_PRCP, winter_TAVG, HIV.prevalence.rate [REASON] predict the missing values based on average of adjacent state (used when data for the whole state is missing) Feature Selection FS001 [ACTION] rank [REASON] using random forest, rank the features based on their importance FS002 [ACTION] rank [REASON] using correlation and entropy FS003 [ACTION] rank [REASON] create linear model and repeatedly remove the feature that contributes the least (or the most) to the model and rebuild the model, rank the features based on the order or removal FS004 [ACTION] rank [REASON] using various attribute search technique to build the best linear model, rank features based on the number of times they appear in the best models FS005 [ACTION] rank [REASON] using various attribute search technique to build the best decision tree (CART), rank features based on the number of times they appear in the best trees FS006 [ACTION] principal component analyze [REASON] using both correlation and ranking score to determine features to be removed and proceed using PCA to reduce these weak features to fewer principal components 2.2 Handling Outliers First of all, since we collect the data from very reliable sources, it means that these data sets have probably been cleansed and checked for error several times by previous compilers. Nevertheless, we cannot just base on that and skip this step. However, since the data set covers a large domain of knowledge, from election, finance, to healthcare, crime, and even weather, we as the author feel that we lack domain-specific knowledge to determine the best range for each feature. As such, we just base on how the values are computed to set the upper and lower bounds that we consider reasonable. For example, a measure of percentage should be limited between 0 and 100, a ratio must be limited to the range of 0 and 1 and a count must be non-negative but has no upper bound potentially. Obviously, for certain feature, like total population for example, we can choose the total population of the country as the upper bound but this is quite meaningless to do, if we really want to drill into the detail, we can set the limit of the county based on the state, but this is like re-inventing the wheel as the state data was essentially just an aggregate of the county data. As such, we just put in very general bounds. There is, however, an exception to this: we do assert the upper bound for the state FIPS code to be 57 since we do not want to include the voting results of outlying areas and freely associated states. Based on these upper and lower bounds, we scan through all the features to see if any of them having values beyond the bounds (HO002). We found out that only the derived feature Voting Participation percentage has outliers (exceeding the upper bound of 100%). This is made possible since when deriving this feature, we divide the number of votes of 2012 and the voting population of the county in 2015: there might be a chance that there is a significant number of people relocating out of the county. The number of violation case is just 3 so we just clamp these values down to 100%. We are neither comfortable with conducting a clamp transformation using the interquartile range because this would affect a large portion of many features, nor willing to try clamping using 2\\(\\sigma\\) range from the mean since many of the features are right-skewed as mentioned in the DQR. Second, we tried to make use of the estimated number of votes to find counties where the result is unreliable, by this we mean counties where we cannot tell who is the clear winner from the current vote count. This procedure (HO001) involves calculating the potential vote gained (in the remaining number of votes) by each major party based on the ratio vote of the county in 2012, then we see if the winner of the county changes when these potential votes are added to the current vote count. This method is arguably based largely on the assumption that each county has tendency to vote for one major party. Nevertheless, its validity is not of great concern here since this method only detects one of such “flipable” county; and therefore it should not affect subsequent steps greatly. 2.3 Feature Trimming Name of the county (name_16) needs to be trimmed (TR011) for it is not useful at all for the modelling process. Of course, being a county that has always been voting for the Republican is really a big thing but perhaps, we should not go into that level of granularity. We will just consider the state as we feel that state fulfills the above-mentioned role of county name and also has a more general implication. Perhaps, a certain county is also affected by its neighbors and being a county in a state that has been known as Red or Blue state for years might affect the way people vote. Next, we will trim the group of election result (TR012). As mentioned briefly in Section 1.4, Data Exploration, features in this group are highly correlated. Figure 2.1: Election Features Correlation Plot Matrix We especially concern with this group because it is where our potential target feature lies. As mentioned before, vote fraction typically lies in the range of 0.4 to 0.6 and thus will not be a good target feature because for regression learners, we would want a larger range of value and the external bias on our part is that we have a better and larger pool of classification learners to choose from. As for the vote count, since it depends greatly on the population and/or voting population, it is even less reliable than vote fraction. For all of that reason, we decide to choose the binary feature that indicates whether Republican won a county as the target feature, i.e. rep16_win is the target feature. We also want to keep such binary indicator of the winner for past elections. Therefore, we just finalize the form of our study to a 2-class problem. Now, we can see that result of 2008 and 2012 are highly correlated (not surprisingly), we can just keep the 2012 result. Also, we want to remove the total population, since we already have a better indicators, such as the population density and the voting power. We should see that in the process of learning process, if we leave the total population in our feature space, a single big county in Illinois may make other county’s values for that feature become minuscule after normalizing. We will talk more about our rationale behind keeping all features as ratio in the article. Therefore, we trim rep08_win and Total.Population (TR013). Last but not least, we get rid of est_votes_remaining and voting_age_population (TR014) as they are only kept to be used to search for outliers. 2.4 Handling Missing Data Based on the summary table in section 1.4, we can see that Homicide Rate and Infant Mortality have percentage of missing value greater than 60%. Although this might seem sufficient to just remove these attributes, we find that these are rather crucial indicators in the healthcare and crime category respectively so we will see what we can do to repair them. We come up with several methods that we can conduct in sequence to fill up the data. Their brief descriptions have been listed above in the Overview section. Here we will provide details on how they are done as well as the rationale behind each method and most importantly, the order in which they are carried out. First, we attempt to look at the past data to predict the current data of a county (HM001 and HM002). To do this, we first look at the NA’s of the current data set for a particular feature. For example, let us look at Teen.births (in HM002), in the pre-process phase, we have updated this feature to the 2016 data (UP001). Nevertheless, there are still many missing values. We then look at the available 2015 result for each county where the 2016 value is NA. We refrain ourselves from taking directly the 2015 results because this act of laziness potentially renders the whole feature useless. Instead, we calculate the average change for all counties in the same state as the concerned county and add that to the 2015 result of the concerned county to compute the predicted value for 2016. If 2015 result is not available, we simply go further to 2014 result to compute the predicted result for 2015 and then the predicted result for 2016. Nevertheless, there are counties where even past results are not available, those will still be NA’s by the end of this procedure. Second, for data where past values are not available, we attempt to predict the missing values based on their location. For geographic data, like winter_PRCP, and winter_TAVG, we can take the average of surrounding counties’ data (HM003) using the county adjacency dataset. However, there are counties where all adjacent counties having missing values, and so this procedure cannot help to predict their values, unless we repeat this process multiple times, which we choose not to because we feel that predicting values based on the result of multiple levels of prediction is not particularly an attractive idea. As for demographic data, in categories such as healthcare and human-development, we should respect the state boundary and thus, we take the average of the data from counties within the same state (HM004). Here, we take the assumption that we can treat the weather data like any other features in the demographic group to predict the missing values based on counties of the same state. After this procedure, for a feature, the only missing values that remain must be from counties whose state has missing values for all of its counties. Here is where we have to use our penultimate resort by taking the a average from adjacent states (HM005) using the state adjacency dataset. Fortunately, this method ensures that that we do not have resort to taking the national average. Nevertheless, we were encountered with a single case where the concerned state has no neighbor, i.e. Hawaii, so we have to take California as the neighboring state. Although this might seem arguable, it actually only affects the weather features and the HIV Prevalence rate slightly so we decide to proceed that way. 2.5 Feature Selection Since we are not very clear about which model we will use for this particular study as we have an extremely diverse attribute space, we will proceed with scheme-independent feature selection. Our aim is to reduce the number of feature down to 40 to make the model simpler. Also, as we have seen in the correlation analysis, there are features to be removed or groups of features to be reduced. So we will try various technique to rank the importance of each feature (essentially scoring them based on their importance) and finally using that scores in tandem with our correlation analysis to select best features. First, we use various technique such as RandomForest to find the weights of the attributes (FS001). The higher the weights, the more important the feature is. We proceed to use the algorithm cfs from FSelector package to find attribute subset using correlation and entropy measures for continuous and nominal data respectively (FS002). The algorithm uses best first search so we keep removing attributes which are selected and run the algorithm on the rest of the feature space. We rank the earliest features to be removed the most important. Then we move on to using an algorithm with very similar fashion, recursive feature elimination (Witten et al. 2017). We builds linear model using the current feature space and remove the feature with lowest coefficient and repeat the process. We also use a variation of this technique by removing the feature with highest coefficient (FS003). Second, we use feature space searching technique to rank the feature. Due to time limit, we resort to not using exhaustive search! Instead, we use various forms of greedy search: forward, backward, best-first, hill-climbing and so we have to repeat 5 times for each search and also to proceed with 5-fold cross validation to reduce the effect of the random split. We uses linear regression to build linear model (FS004) and CART to use decision-tree classifier (FS005) to build the model since these are typical choices of algorithm for scheme-independent feature selection (Witten et al. 2017)–for their light-weightedness and simplicity. The obtained result are formula to build the best model using the given algorithm, we count the number of times each feature appears in these formula and rank them by the frequency of their presences. In the midst of this process, we can see that the result of 2012 appears in almost every subsets, and evidently, the result of 2012 is highly correlated to the result of 2016, so we also try to run the searches without the 2012 result. Since we are dealing with a 2-class problem, we also tinker a little bit with the searching criterion. Accuracy comes as a standard choice but the Kappa value also makes a lot of sense in this case especially when we have many features, the chance of having a model that does little better than a random predictor does exists. We feel that other metrics such as recall or precision, i.e. the rate of predicting correctly which county Republican won and not caring about which state they lose, does not seem to be sensible options in this case. The detail result for each procedure can be found at /result/data_feature_rank/ After all of these steps, we compile the final score for each features; and present them as below. The state FIPS code is the only nominal features and thus could not be used for linear regression, but in other procedures, it performs well, as such, we will not remove it. As for 2012 result, as noted above, it is a strong predictor for the 2016 result and so we might consider leaving it out. But since it helps answer one of our original question, we choose to leave it in for now. CFS : corelation and entropy based method RFE : recursive feature elimination RF : random forest AS.LM : attribute space search using linear regression AS.Tree : attribute space search using CART Table 2.1: Feature Score Name CFS RFE RF AS.LM AS.Tree Score White not Latino Population 49 52 51 32 47 231 Graduate Degree 49 45 49 36 48 227 African American Population 49 50 50 38 36 223 Voting Power 51 46 44 34 26 202 At Least Bachelor Degree 49 40 48 24 38 198 Production, Transportation, and Material moving Occupations 48 36 40 28 46 198 Latino Population 48 46 46 26 31 196 Homogeneity Index 48 34 45 42 26 195 Sexually Transmitted Infections Rate 49 22 42 42 37 192 Population Density 50 26 47 32 36 191 Asian American Population 49 48 39 29 26 191 Age Dependency Ratio 49 35 31 36 33 184 Adult Obesity Rate 49 34 43 27 28 181 Other race(s) Population 49 48 20 39 22 178 Homicide Rate 48 35 30 24 37 174 Diabetes Rate 49 40 34 23 28 174 Children in Single Parent Households Rate 49 26 41 30 25 172 Median Age 48 15 37 32 38 170 Management, Professional, and Related Occupations 48 45 27 28 23 170 Gini Coefficient 49 22 38 40 19 168 Service Occupations 48 32 25 32 27 164 Construction, Extraction, Maintenance, and Repair Occupations 49 30 35 28 22 163 HIV Prevalence Rate 49 30 28 32 12 152 Low Birth Weight Rate 48 29 18 21 34 150 Voting Participation Rate 47 31 23 19 30 150 Winter Avg. Precipitation 46 13 36 27 27 149 Less Than High School 47 18 29 31 22 147 Winter Avg. Temperature 46 15 24 28 27 139 Injury Death Rate 48 26 22 15 27 138 Farming, Fishing, and Forestry Occupations 48 26 14 18 32 138 Sex Ratio 48 18 10 39 21 136 Unemployment Rate 48 22 4 34 27 135 Uninsured Rate 49 14 26 13 31 133 Avg. No. of Physically Unhealthy Day 46 16 11 25 34 132 Adult Smoking Rate 48 22 5 33 21 129 Native American Population 47 34 3 20 25 129 Crime Rate 48 26 16 18 21 128 Infant Mortality 47 12 6 29 34 128 Life Expectancy 48 22 21 19 18 128 School Enrollment 46 9 12 37 21 125 Teen Birth Rate 47 9 19 34 14 123 Median Earning 48 22 15 25 11 121 Sales and Office Occupations 47 25 7 23 18 120 Avg. No. of Mentally Unhealthy Day 45 20 2 30 24 120 Children Under 6 Living in Poverty 47 16 9 23 23 118 Poverty Rate 48 15 17 21 16 117 Child Poverty 48 14 13 16 26 117 Adults 65 and Older Living in Poverty 47 12 33 8 15 115 Preschool Enrollment Ratio 48 12 8 14 33 114 Republican Win 2012 52 41 52 NA NA NA FIPS State Code 47 NA 32 NA 13 NA 2.6 Feature Space Reduction After we have obtained the score for each feature, it is time we considered what features we should remove. Well, we can naively cut the bottom 10 features or so, but this is clearly not the optimal approach because of the following 2 reasons. First, attribute search space and cfs uses greedy search approach and thus, the result is affected by the random starting node. The best way is to conduct an exhaustive search but this is too expensive and the underlying algorithm is CART and linear regression which, due to language bias, might not be able to excavate the underlying function. Also, the result we got is aggregation of different methods, which makes it tricky to actually select the “worst performing features.” Second, there could be situation when a feature alone might not be a good predictor but a group of feature including that low-coring feature forms a strong indicator. This we might not know until we try to build model using more sophisticated algorithm. As such, the crude method of trimming features like we did before for the case of the election group is not reasonable here. As we have mentioned before, we can also look at the correlation to get rid of the some of the features. Disregarding the case of 2016 and 2012 result, we list the top correlation between the all the features we have in our current feature space below. A quick glance at the top correlation results remind us of the second strongest correlated groups of feature that we mentioned before: healthcare and finance (poverty in this particular case). It is not hard to see how these 2 categories of features are closely related. This might be useful for our feature selection process as many healthcare and poverty feature performs poorly in the feature ranking phase. However, we want to demonstrate that to base solely on correlation to discard a feature is not reasonable enough. For example, consider the second most correlated pair, i.e. “At Least Bachelor Degree” and “Graduate Degree”, the correlation between these 2 features are expected but they imply different things, and may contribute very differently to the underlying function. Back to the healthcare and poverty feature group, we can list out several of them that are low-scoring but we cannot point out exactly which to remove between them, nor can we remove the whole group altogether! In this kind of situation, what we can do is to use principal component analysis (PCA) to reduce this group of features into less features but these derived features can be used to represent the original features. As such, we construct a graph of correlation between feature and puts an edge between features that have absolute correlation value greater than 0.65. Next, we take away nodes which represent features having score higher than average. Figure 2.2: Highly-correlated and Low-Scoring Features Afterwards, we make a breadth-first search through the graph starting from the node with highest degree to find the cluster of nodes we should consider to run PCA on. The early stopping criteria for a search is when the branch ends with a node of degree 1, in other words, we are trying to find a dense cluster of nodes. The nodes we obtain in the end represent the group of feature that are low-scoring but highly-correlated. We can then pass this group of features to run PCA on. The PCA result we obtain is presented below. To select the right number of components, we can either sketch out the Scree Plot (Brown 2009) or look at the percentage of cumulative variance plot. The former allows us to see, based on the slope, which components contribute the most to the total variance whereas the latter allows us to know how many features we want to choose so that we can account for a certain percentage of the total variance (Witten et al. 2017). Although based on the position of the “knee” in the Scree Plot, we can conclude that we only need the first component (PC1), we think that we should go for the “safer” option by choosing the first 4 principal components which account for 90% of the total variance. Our reason is that PC1 only covers 67% of the total variance, and this value is too low for us to consider PC1 as the lone replacement of the original 10 features. Notice that unlike factor analysis, we do not really try to derive any meaning out of each component; components found using PCA are merely for calculation purpose and so the coverage really matters in this case. Figure 2.3: Principal Component Variance Dsitribution To demonstrate our point that the components in PCA do not give us a clear interpretation, let us consider Figure 2.4 (due to the limit of current visualization technique … we cannot really show all 4 components). Clearly, we can see that PC1, the most important component, is contributed by many original features so it is hard to tell what it really represent; as for PC2, perhaps we can conclude that it represents the group of health-related features). So the downside of using PCA is that we might lose some clarity in the interpretation of the model. Nevertheless, since our goal is to gain business understanding of the result, we need to be able to have a rough understanding of the chosen components. Refer to PC rotation table above, we conclude that PC1 represents a measure for general healthcare quality received by the low-income population since based on the rotation, it is contributed rather equally by poverty and poor healthcare. PC2 represents a measure for adult smoking-related health problem rate. PC3 is rather pure as it depends greatly on child poverty rate. PC4 is also contributed mainly by teen birth rate. Setting up 3D graphics has always been a struggle (the great value is that you can try to rotate and zoom). So if you cannot see this widget, you can enjoy the 2D version here Figure 2.4: Principal Components and Original Features The final step of this process is to remove the 10 low-scoring features and replace them with the 4 new principal components to obtain the Analytic Base Table, which we will present in the very next section. 2.7 Analytic Base Table (ABT) This is the data set that we will use for the modelling process. Take note of elec_rep12_win and state_fips features. Their presences correspond to our first and second question in the introduction respectively. If we include the 2012 result, we are incorporating historical feature into our modelling, if we incorporate the state, we are incorporating the geographic factor into our learning process. Although there are 2 other geographic factor related like weather but we feel that weather varies dramatically within one state that it is really hard to say if weather can replace state as a the main geographic factor or not. Also, state is intrinsically a composite feature that covers beneath it more than just geography, but way beyond, including history, socioeconomic, etc. As such, if state is included in the modelling process, it is also quite tricky to interpret the learned model. We will talk more about this in Section 3 “A Brief Note on Modelling.” With all that being said, we present the Analytic Base Table below. Figure 2.5: Analytic Base Table Again, to visualize this data set better, we present the following choropleth. References "],
["md.html", "Section 3 A Brief Note on Modelling 3.1 Overview 3.2 Classifiers and Tuning Parameters 3.3 Addressing Imbalanced Class Distribution", " Section 3 A Brief Note on Modelling 3.1 Overview In this section, we will briefly go through our selection of classifiers. More than just that, for each model, we want to investigate which parameters we can tune to optimize the performance; and for each parameter, we want to obtain a reasonable range of values so we can automate the tuning process. Also, each model has its odds and ends. At least, since we have dealt with all the missing values in our data in section 2, we just need to care about the type of the descriptive features and the target (nominal or continuous). Besides, each modal potentially requires different data pre-process procedures. Also, for each tune, we run cross validation to minimize the effect of random splits. Last but not least, we take the huge difference between the number of counties won by the Republican party and that of the Democratic party into account, and so we can either under-sample or over-sample the training set. In the next part, we list out the main procedures we use in the modelling process. We use the term “probing” to describe the procedure in which we identify the range of values for the parameters of computationally expensive classifiers, such as K-Nearest Neighbor and Neural Network. Essentially, we perform a preliminary tuning for these classifiers. For the data set used, We feel that we should not include the 2012 results as well as the state FIPS code to avoid any effect that these two features might contribute to the performance of the data and to make the probing process faster due to the lower number of features. The final procedure is to do a quick exhaustive search for the subset of categories from which we can generate the best model out of. The result of this process as well as the result of the modelling process in general will be discussed in detail in section 4. MD001 [ALL] all except elec_rep12_win and fips_state [ACTION] tune [REASON] probe over a large range of values to see the suitable range for tuning the number k of nearest-neighbor classifier (knn) MD002 [ALL] all except elec_rep12_win and fips_state [ACTION] tune [REASON] probe over a large range of value to see the suitable range for tuning the number k of weighted nearest-neighbor classifier (kknn) MD003 [ALL] all except elec_rep12_win and fips_state [ACTION] tune [REASON] probe over a large number of possible hidden layers configuration for neural network MD004 [ALL] all except elec_rep12_win and fips_state [ACTION] tune [REASON] visualize the performance of the resuling model to identify the best tuning range for the computationally expensive algorithms MD005 [ALL] all [ACTION] tune [REASON] tune parameters for all the classifiers MD006 [ALL] all except elec_rep12_win and fips_state [ACTION] rank [REASON] attempt to find the best subset of categories/domains that could have been used for modelling 3.2 Classifiers and Tuning Parameters Since this is a classification problem, tree-based and rule-based classifier naturally come to our mind. We will use 1R as our base classifier, and also try Naive Bayes as a representative of probability-based classifier. In terms of linear regression, we choose Neural Network, Support Vector Machine (SVM) and a simple modified version of linear regression that can work with nominal target. The modified version of linear regression involves choosing a threshold to decide the class of the target value. Regression tree, however, is not considered because the process of building the tree often looks for minimizing the variance at the leaves (Witten et al. 2017), which is in no way compatible with our typical performance metrics such as accuracy and Kappa value. Lastly, we also include some instance-based learners for the sake of completeness. In the following sub-sections, we will go into detail of each group of classifiers. 3.2.1 Base Classifer Classifier that is really simple or constructed based on naive assumption on the data set. They generally works decently in most cases so they can be used to produce lower bound for the performance of more sophisticated classifiers. A typical use of this lower bound is to detect drastic reduction in performance of other classifiers while tuning, which potentially suggests over-fitting. 1R A simple rule-based classifier that only attempts to build 1 rule for the splits on one descriptive feature. It makes the assumption that there exists one feature that can be used to predict the target really well. As such, we can expect OneR to do well when we include strong predictor like the 2012 result. 1R serves well as the base classifier whose model’s performance can serves as the lower bound for other more sophisticated models. 1R only works with nominal descriptive features (DF) and target feature (TF), so we need to bin our the numeric features in our data set. We, however, can use continuous features with the implementation RWeka::OneR as it has a built-in supervised discretization mechanism (Witten et al. 2017) which allows user to specify the minimum bucket size for discretization. We will, however, use the default supervised discretization function in RWeka developed using Fayyad &amp; Irani’s MDL method. Since this is just the base classifier, we will not spend a great deal of effort trying to tune it. Summary Algorithm: OneR Package: RWeka Input Requirement: Descriptive Feature: nominal Target Feature: nominal Other: none Tuning Parameter Binning method (oner.bin) Number of bins (oner.nbin) 3.2.2 Probability-based Classififer Classifier that constructs the a set of conditional probability and priors for features value to calculate the value of the target feature. Usually, we will include Naive Bayes as a base classifier but we have done prior exploration and Naive Bayes yields amazing good results which makes us decide to separate it from 1R. Since the produced model is a set of probability, interpretation might be quite challenging. Naive Bayes This is the only probabilistic method used. It takes the “naive” assumption that every features contribute equally to the result and it calculates the conditional probability of each feature to predict the value of the target feature. As such, the resulting model will be in the form of a set of table of probabilities for each feature. It works well with continuous DF but the worry that we have is that Naive Bayes assumes normal distribution for the continuous features so it can calculate the conditional probability of a novel instance using the probability density function. Most of our DFs have very skewed distributions, so we think it is best to force discretization on all features. As of course, similar to 1R, when it comes to discretization on heavily skewed data distribution, the best way is to avoid equal-interval discretization and to use either supervised binning or equal-frequency binning. The Naive Bayes implementation from e1071::NaiveBayes also supports Laplace smoothing (to avoid absolute 0 probability) so we can vary the value of this estimator in our tuning process. Summary Algorithm: NaiveBayes Package: e1071 Input Requirement: Descriptive Feature: any Target Feature: nominal Other: none Tuning Parameter Binning method (nb.bin) Number of bins (nb.nbin) Laplace estimator (nb.laplace): a positive double controlling Laplace smoothing. The default (0) disables Laplace smoothing. 3.2.3 Rule-based Classifier Classifiers that produce rule set: particularly, in this case, we have both PART and RIPPER producing decision lists (sets of ordered rules). Rules can be more comprehensive than most of the other models (arguably better than decision tree) so even if the result of models built by these classifiers are not the best, we can still based on them to learn about the data set. PART A rule learning classifier that combines the divide-and-conquer strategy of decision-tree with separate-and-conquer strategy of rule learning. It builds partial (C4.5) decision tree on the current set of instances and makes the “best” leaf into a rule. The partial tree is then discarded and the instances covered by the rule is then removed. The process repeats until all training instances are classified. It works well with any type of DF. For this particular implementation RWeka::PART, we can change the minimum number of objects per leaf. We can also choose if we want to enable reduced-error pruning, otherwise, we can specify the confidence threshold for pruning, set by default at 25%, which works well in most case (Witten et al. 2017). Summary Algorithm: PART Package: RWeka Input Requirement: Descriptive Feature: any Target Feature: nominal Other: none Tuning Parameter Minimum number of objects per leaf (part.minLeaf) Enable reduced-error pruning (part.REP) Confidence threshold for pruning (part.pruneConf) RIPPER Repeated Incremental Pruning to Produce Error Reduction (RIPPER) is a very popular rule extraction algorithm since it scales linearly with the number of instances and works well with imbalanced class distributions. It predicts the most frequent (majority) class and learns rules for the less frequent ones, starting with the least frequent. It uses the general to specific approach: growing one rule by greedily adding antecedents (or conditions) to the rule until the rule is perfect (i.e. 100% accurate). The procedure tries every possible value of each attribute and selects the condition with highest FOIL’s information gain. It stops adding conjuncts if the number of negative examples covered increases. It uses reduced-error pruning to post-prune added conjuncts in reverse order. The specification of this algorithm is very similar to that of PART. However, for this implementation RWeka::Jrip, we can only modify the minimal weights of instances within a split. Summary Algorithm: JRip Package: RWeka Input Requirement: Descriptive Feature: any Target Feature: nominal Other: none Tuning Parameter Minimum weight of instance within a split (jrip.minWeight) 3.2.4 Tree-based Classifier The learning approach which builds decision tree as the model. It is as interpretable as rule-based models, but it has the advantage in that it is much easier to visualize. We choose the classic C4.5 algorithm and the CART algorithm. Tree-based classifiers like rule-based are non-parametric and thus does not assume any data distribution. As such, they are good choices for our data set. C4.5 An improvement of ID3 algorithm as it uses gain ratio to determine which feature to split on instead of using information gain, which is very susceptible to features with high cardinality. It supports continuous features, as it looks for optimal threshold to split the instance: it sorts the instance by the value of the DF and considers the midpoint between target value change as potential split point, for each of which it calculates the gain ratio to choose the best split point. Like the PART implementation, RWeka::J48 allows us to choose if we want to enable reduced-error pruning as well as to modify the minimum number of objects per leaf. Summary Algorithm: J48 Package: RWeka Input Requirement: Descriptive Feature: any Target Feature: nominal Other: none Tuning Parameter Minimum number of objects per leaf (j48.minLeaf) Enable reduced-error pruning (j48.REP) Confidence threshold for pruning (j48.pruneConf) CART This is an algorithm that uses recursive partitioning for classification. This implementation rpart::rpart allows us to modify various parameters including the complexity parameter (cp). Any split that does not decrease the overall lack of fit by a factor of cp is not attempted. There are other approach offered by the implementation as an alternative to this early-stopping criteria such as changing number of observations that must exist in a node in order for a split to be attempted or the minimum number of observations in any terminal (leaf) node. But we feel that using cp is a characteristic value for this algorithm as we can either pre-prune or post-prune just by changing the the cp value. Summary Algorithm: rpart Package: rpart Input Requirement: Descriptive Feature: any Target Feature: nominal Other: none Tuning Parameter Complexity parameter (rpart.cp) 3.2.5 Regression-based Classifier Regression-based classifier attempts to put weight on each features to come up with a linear formula to compute the numeric value of the target. It can also be used for classification problem (e.g. logistic regression, supporting vector machines, etc.) to find the hyper-plane that separates the assumed linearly separable instance space. They general work well and rather robust, but some like neural network is too computationally expensive. Another downside of this approach is that for algorithm that only outputs numeric values for the target, we have to choose a threshold for the data to turn regression results into classification results, and lastly, the resulting models come as a set of weights, which is really hard to interpret. Since the underlying mechanism is to put weights on each feature, regression-based are susceptible to data with different scales so we will normalize or standardize all numeric features. Fortunately, binary features naturally fall into the range [0,1] like normalized features do. Linear Regression The classifier we choose here is actually least-squares linear regression which attempts to minimize the sum of the squares of the differences over all the training instances by choosing weights for the linear combination of all features. For the native implementation of R, i.e. base::lm, we only tune the threshold which is used to determine the class of the predicted values. Since this algorithm only works for continuous features, we have to turn nominal features into binary, i.e. the state FIPS code must be turned into multiple binary features. The downside of this, as we have discovered is that the learned model does not end up using all of the features, resulting in many warnings from R with “rank-deficient fit.” Essentially it means that one or more feature can be be expressed as the linear combination of other features so they are discarded while building the model. There is also a popular misconception that linear model is parametric, which means they assume normal distribution of the input. But as pointed out, the assumption on distribution only applies to the error, not the data itself (Williams, Gómez Grajales, and Kurkiewicz 2013), so we can safely use linear regression without worrying to much about transforming the distribution of the features. Summary Algorithm: lm Package: base Input Requirement: Descriptive Feature: continuous Target Feature: continuous Other: normalize Tuning Parameter Decision threshold (lm.threshold) Support Vector Machines (SVM) The algorithm tries to find the best hyper-plane that separates assumed linearly separable instance space. The implementation e1071:svm supports classification and allows us to modify the kernel used in training and predicting as well as cost of constraints violation, i. e. the C-constant of the regularization term in the Lagrange formulation. In essence, the constant tells SVM optimization how much we want to avoid mis-classifying each training instances. For large values of C, the optimization will choose a smaller-margin hyper-plane if that hyper-plane does a better job of getting all the training points classified correctly. Conversely, a very small value of C will cause the optimizer to look for a larger-margin separating hyper-plane, even if that hyper-plane mis-classifies more points. Since our knowledge on the various kernel type is limited, we will not attempt to tune on the parameters that correspond to each kernel type but using the default values instead. Another issue is that we have to scale/standardize the features since it helps “avoid attributes in greater numeric ranges dominating those in smaller numeric ranges and numerical difficulties during the calculation” as “kernel values usually depend on the inner products of feature vectors, e.g. the linear kernel and the polynomial kernel, large attribute values might cause numerical problems” (Hsu, Chang, and Lin 2003). Since SVM is sensitive to imbalanced class distribution, we will also adjust the weights if training set is left unsampled by adjusting the parameter class.weights in the e1071::svm implementation. Summary Algorithm: svm Package: e1071 Input Requirement: Descriptive Feature: continuous Target Feature: any Other: standardize Tuning Parameter Kernel type (svm.kernel) Cost of constraints violation (svm.cost) Neural Network This is probably the most advanced (as well as computationally expensive) algorithm used in this study. Neural Network seeks to chain up multiple layers of perceptron/components that assign weights to its inputs for computing outputs, in order to optimize the performance. As such, in terms of tuning, for this particular implementation neuralnet::neuralnet, we have the freedom to choose the shape of the hidden layers. As more sophisticated configurations (2 layers or more) often require more time, we have to raise the initial limit on the maximum number of step from 100000 to 10000000 to make sure the algorithm converges. We decide not to try beyong 3 layers as we have tested with 3 hidden layers and this take brutally long to finish and we can see that those complicated neural network do not always perform better. Also because 2 layers are often sufficient to generalize well (Witten et al. 2017) (though again, data mining in general tackles ill-posed problems, there is no guaranty that a model really expresses the true underlying function), and 1 layer with 1 node merely resembles the structure of linear regression approach, we will tune in the range from 1-hidden-layer with multiple nodes to 2-hidden-layer with various node configurations. The implementation also allows modifying the algorithm. The default is RPROP, with 2 flavors rprop+ and rprop-, which stands for resilient backpropagation with and without weight backtracking. We have tried to work with the normal backpropagation method but this requires specifying the learning rate which differs in each case. Resilient backpropagation modifies the learning rate automatically so we can avoid getting stuck due to inappropriate learning rate assignment. Last but not least, since neural network outputs numeric values, we set the decision threshold to 0.5. Summary Algorithm: neuralnet Package: neuralnet Input Requirement: Descriptive Feature: continuous Target Feature: continuous Other: normalize Tuning Parameter Algorithm (neuralnet.algorithm) Hidden layer(s) configuration (neuralnet.hidden) 3.2.6 Instance-based Classifier This class of classifiers are often called lazy-learners since they store the training set as the model and predict novel data by comparing them to the existing training instances. Here, we choose to use K-Nearest Neighbor which will determine the class value of the novel instances based on its k nearest instances in the instance storing data structure. Since the whole idea of the algorithm is to find the closest neighbors, it then must have a distance metric. This works best when all features are of numeric type (nominal features result in very crude distance measure such as 0 or 1) and when they have appropriate scales (Witten et al. 2017). As such, it is best to normalize all numeric features. K-Nearest Neighbor For this particular implementation class:knn, we can only modify the number of neighbor k. However, there is a known bug, which limits the size of k to less than 500. Of course, when we increase k to 500, that pretty much defeats the purpose of this algorithm, we want to find the optimal value of k that is not so large (to make classification works relatively fast) but still large enough to create model that performs well. Another consideration that worth mentioning is that for unweighted implementation, for a 2-class problem, even value of k often yields less optimal result because ties in vote can occur, as such, we can optimize our tuning process by just considering odd values of k. Summary Algorithm: knn Package: class Input Requirement: Descriptive Feature: continuous Target Feature: nominal Other: normalize Tuning Parameter Number of nearest neighbor (knn.k) Weighted K-Nearest Neighbor This is the weighted version of the basic K-Nearest Neighbor algorithm. This implementation kknn::kknn allows us to modify not only the number k of neighbors but also the kernel type used and the distance metrics. Despite this degree of freedom, we choose not to tune these 2 values but to use the default because this particular algorithm is rather computationally expensive and we have to tune for the best k value in a wide range of k. The default metric distance is 2, which is equivalent to Euclidean metric in Minkowski distance class; this to us is a fair choice. As for the k parameter, as each neighbor now weighs differently, we have to consider even value of k as well. Summary Algorithm: kknn Package: kknn Input Requirement: Descriptive Feature: continuous Target Feature: nominal Other: normalize Tuning Parameter Number of nearest neighbor (kknn.k) 3.3 Addressing Imbalanced Class Distribution As noted before in section 1.4, we have an imbalanced class distribution for our target feature. This might hurt some of the classifiers we are about to use. This imbalance affects the estimate of the intercept of linear models (King and Zeng 2001). It also affects tree-based model (Cieslak and Chawla 2008) and neural network unless cost-sensitivity is introduced to the learning algorithms (He and Garcia 2009). As such, during the training process, we will proceed with either under-sampling or over-sampling. Nevertheless, we will still try our luck with the original data as we know some classifiers we use also deal well with imbalanced distribution, i.e. rule-based classifiers such as RIPPER, since they employ the separate-and-conquer approach which only targets at only one class at a time. PART is an exception here as it attempts to build the partial tree first so it might be at a disadvantage while working with imbalanced dataset. The SVM algorithm we use is from the package e1071 which uses the engine of LIBSVM, which supports weighted SVM for unbalanced data. Each sampling strategy, however, has its own quirks. Under-sampling requires us to remove instances of the majority class to match the number of instances of the minority class. Throwing away real data is never a great option. In turn, the size of the data set shrinks and this costs models learned using this training set more susceptible to the effect of the random split. So we have to increase the number of fold for cross-validation. Over-sampling, on the other hands, involves boostraping data with replacement to boost the number of minority class instances to match that of the majority class. Duplicating data is also not a great option neither since it randomly duplicates the minority data instances. This does not gain us any information, but also potentially leads to over-fitting (Ganganwar 2012). This also significantly increases the size of the data set, making computation slower and thus, we decide to reduce the number of fold in cross validation. Over-sampling has a slight advantage over undersampling in justifying for our reducing number of fold in cross validation in that it has bigger training set, and no instance is thrown away, as such, it might have slightly better performance. Nevertheless, for both under-sampling and over-sampling, we risk increasing the effect of outliers. These trade-offs are clearly significant and we cannot really tell which is straight-out better than the other. As such, in the evaluation phase, we will compare these on case by case basis. References "],
["rep.html", "Section 4 A “Minority” Report 4.1 Introduction 4.2 Methodology 4.3 Data Manipulation 4.4 Modelling 4.5 Result and Evaluation 4.6 Discussion 4.7 Conclusion 4.8 Further Thoughts", " Section 4 A “Minority” Report 4.1 Introduction In this study, we attempt to analyze the result of the 2016 US presidential election. To many, the result appears to be a very unpredictable, but is it really the case? Our main objective is to understand how various demographic, geographic and historical factors help shape the election results. Historical factors, i.e. the recent election results, are often strong predictors for the outcome of the current election since we usually do not expect to see a drastic change in the political view over a short span of time. Geographical factor, such as state, is also a good predictor in the exact same way. As such, we want to approach the problem in hierarchical manner. Initially, we will consider the effects of all factors on the result. Second, we will leave historical factor(s) aside and proceed with just geographic and demographic factors. Finally, we will just consider demographic factors. We feel that geographic indicator, such as state, is a complex composite indicator that potentially encodes unique information on various demographic aspects; for example, there are states we know as blue state, as rich state, as highly-populated state, etc. Therefore, it is not really correct to refer to this as geographic factor. Our main objective is then to find a model that can predict the result given purely demographic factors. We find that this will give us more interesting learnings than when we include state and past election results. 4.2 Methodology We will follow the CRISP-DM model, starting from building up our domain knowledge, gathering and pre-processing the data set that we have obtained (section 4.3.1). We will spend time explore and attempt to understand the data set better (section 4.3.2) as this will help us in the next phases. Then we will go through the data preparation phase (section 4.3.3) where we select the most relevant features to finalize our analytic base table (ABT), which will be fed as input for the modelling process (section 4.4). Evaluation will then be conducted to choose the best models (section 4.5); and finally in the deployment phase (section 4.6), we will interpret these models in order to achieve our objectives. At the end of this article, we will mention several considerations and improvement that we have for further development of this project. 4.3 Data Manipulation 4.3.1 Compilation and Pre-process The original data set is collected by Emil O. W. Kirkegaard from various sources for usage in his socioeconomic research (Kirkegaard 2016). When Emil published his data set, he also includes the election results for 2008. 2012, 2016 from the New York Times as well as weather data by county, which is really tedious to collect, from the National Oceanic and Atmospheric Administration. Due to its extensive coverage of many demographic indicators, we decide to take this as our starting point from which we will add on more relevant data in order to obtain the Analytic Base Table (ABT). There are quite a number of irrelevant features in the original data set, including socioeconomic features, which were derived by the collector and of which we have little understanding. Features which are derived for technical purposes, such as discretized version of an already-present features, or such are discarded. Redundant features that present merely as remnants of data matching processes are discarded as well. We also checked through the data and determined that minor parties did not win any county so we can collectively group their votes together and regard this group as “other”. In terms of weather data, we are most interested in winter weather of each county for this is the time of the election. Last but not least, we remove instances where there are a lot of missing values or simply where we do not have any information on the election result, because these instances are not useful for our study. These includes counties in Alaska where the administrative geographical division is not county but borough, and Oglala Lakota county where the Indian reservation is situated, and thus there were not enough demographic data about this region. We also add in several demographic features that we think might be important yet missing from the original data set, such as sex ratio, age-dependency ratio, life expectancy, etc. We also derive the voting participation rate and voting power based on the value of the electoral vote of the state which the county belongs to. Last but not least, we update several healthcare and human-development features in the original data set using more recent data from the same sources used by the original collector. After this pre-process phase, we obtain a very crude data set containing most of the desired features. We believe that the best way to have a feel for this data set is to visualize it using a choropleth, so we present one below. See Appendix A to view the dataset and Section 1 for the full data quality report and more detail on the data pre-process phase. 4.3.2 Exploration Before proceeding further, we want to gain some understanding of the data set as this will enhance our decisions in the data preparation phase. Since the data set has many features, we prefer correlation plot matrix over scatter plot matrix as this is clearly the better way to visualize our current data set. Fear not the minuscule! Hover on the the plot to zoom Figure 4.1: Correlation Plot Matrix We also make a summary of the current data set including both continuous and categorical features. 4.3.3 Preparation As it can be seen, there are yet many missing values across the features. Also, the feature space is too huge (78 features) for producing meaningful models–as we use these data to construct models, we want to look at the structure of the models to elucidate us on how the election result was influenced by these features, so the more features we have, the more nebulous the learning outcomes become, i.e. we want our models compact. Most importantly, since it is pretty clear at this point that we will conduct supervised learning procedure, we need to find a target feature for our model. As shown in the correlation plot, features in the election result group are highly correlated (Figure 4.2. We especially concern with this group because it is where our potential target feature lies. As mentioned before, vote fraction typically lies in the range of 0.4 to 0.6 and thus will not be a good target feature because for regression learners, we would want a larger range of value. The external bias on our part is that we have a better and larger pool of classification learners to choose from so we really prefer classification problems to regression problems. As for the vote count, since it depends greatly on the population and/or voting population, it is even less reliable than vote fraction. For all of those reasons, we decide to choose the binary feature that indicates whether Republican won a county as the target feature, i.e. rep16_win is the target feature. We also want to keep such binary indicator of the winner for past elections. Therefore, we just finalize the form of our study to a 2-class problem. Figure 4.2: Election Features Correlation Plot Matrix We also just keep the 2012 result as it is highly correlated to 2008 result–not surprisingly, as both elections won by Barack Obama. We also remove features that has identifying nature like name and ID as these are not at all useful for modelling process. We consider features like population or area not useful, as we try to avoid raw-count features and favor ratio and percentage so that we can have a scale of values to work with. Also, by deriving new features from these raw-counts, we might end up with strong features, stronger in the sense that the features will not obfuscate the learning algorithm. For example, population by county differs greatly. A candidate might be interested in knowing that a county is highly populated, but he will be more attracted to county which is more densely populated, or a county where each voter contributes more to the electoral vote of the state, or a county where more people actually go to vote. Knowing that a county has more than half a million people, might be a good thing for contestants of “Who wants to be a millionaire” but perhaps not particularly useful for politician. The advantage of ratio feature over raw-count feature is that it defines a range of possible values for the feature, e.g. 0 to 1 or 0% to 100%. Another advantage is that it gives the lonely figures a context. If a governor hears that there are 9 miscarriage cases per month in a county X he will have utterly no idea is that low or high, but if given that 9 miscarriage cases out of 10 live birth cases, he knows that there is a problem. A point to note is that deriving ratio features does not alter the distribution of the data in anyway. Due to a single extreme value, the ratio feature might end up with 1 instance at 100% and the rest lies within 0-5%. This is something we do not attempt to change, such as to do a log-transformation, because we deem ourselves lacking understanding of the true distribution of many of these features. In short, we trim away features like population, land area, etc. And finally, we get rid of features like estimated vote remaining or FIPS code for county as these are kept purely for data manipulation purpose. As mentioned above, an advantage of deriving ratio features is to have an idea about the lower and upper bound of a feature, based on this, we check for outliers and proceed with clamp transformation on them. We also attempt to find counties with unreliable election result, i.e counties where based on the reported number of votes, number of votes remaining, and the past result, we can expect a swing in the winning party. After this, we attempt to handle missing values. We proceed this in hierarchical manner. First, for features where we have data from the previous year(s), we can predict the currently missing values by adding its past year data with the average net change of counties in the same state with the concerned county. This bases on the assumption that counties in the same state, under the same policies and laws, will have similar changes over the years. We only do this for healthcare and human-development features as we have past data set for them. For geographical features, such as weather, that depends less on administrative policies, we take the average across adjacent counties. The next step is to fill up missing values using the average of counties within the same state and finally average of adjacent states. We are fortunate enough to not have to resort to using national average to fill in these missing values. Finally, we have our data set in good shape, but we need to address the problem of irrelevant or redundant features. We use common scheme-independent feature selection techniques, such as ranking by importance using RandomForest, ranking by entropy for categorical features and correlation for continuous features, ranking by coefficient using recursive feature elimination (Witten et al. 2017). We also try to build decision trees (using CART) and linear models to search the attribute space. We use various greedy search techniques such as best-first, forward, backward, hill-climbing and apply cross-validation minimize the effect of random test-train splits. We set the searching criterion to accuracy and kappa to find the subset of features which usually contribute for the best models. Based on all of these results, we compute a score for each feature and rank them accordingly. CFS : corelation and entropy based method RFE : recursive feature elimination RF : random forest AS.LM : attribute space search using linear regression AS.Tree : attribute space search using CART Table 4.1: Feature Score Name CFS RFE RF AS.LM AS.Tree Score White not Latino Population 49 52 51 32 47 231 Graduate Degree 49 45 49 36 48 227 African American Population 49 50 50 38 36 223 Voting Power 51 46 44 34 26 202 At Least Bachelor Degree 49 40 48 24 38 198 Production, Transportation, and Material moving Occupations 48 36 40 28 46 198 Latino Population 48 46 46 26 31 196 Homogeneity Index 48 34 45 42 26 195 Sexually Transmitted Infections Rate 49 22 42 42 37 192 Population Density 50 26 47 32 36 191 Asian American Population 49 48 39 29 26 191 Age Dependency Ratio 49 35 31 36 33 184 Adult Obesity Rate 49 34 43 27 28 181 Other race(s) Population 49 48 20 39 22 178 Homicide Rate 48 35 30 24 37 174 Diabetes Rate 49 40 34 23 28 174 Children in Single Parent Households Rate 49 26 41 30 25 172 Median Age 48 15 37 32 38 170 Management, Professional, and Related Occupations 48 45 27 28 23 170 Gini Coefficient 49 22 38 40 19 168 Service Occupations 48 32 25 32 27 164 Construction, Extraction, Maintenance, and Repair Occupations 49 30 35 28 22 163 HIV Prevalence Rate 49 30 28 32 12 152 Low Birth Weight Rate 48 29 18 21 34 150 Voting Participation Rate 47 31 23 19 30 150 Winter Avg. Precipitation 46 13 36 27 27 149 Less Than High School 47 18 29 31 22 147 Winter Avg. Temperature 46 15 24 28 27 139 Injury Death Rate 48 26 22 15 27 138 Farming, Fishing, and Forestry Occupations 48 26 14 18 32 138 Sex Ratio 48 18 10 39 21 136 Unemployment Rate 48 22 4 34 27 135 Uninsured Rate 49 14 26 13 31 133 Avg. No. of Physically Unhealthy Day 46 16 11 25 34 132 Adult Smoking Rate 48 22 5 33 21 129 Native American Population 47 34 3 20 25 129 Crime Rate 48 26 16 18 21 128 Infant Mortality 47 12 6 29 34 128 Life Expectancy 48 22 21 19 18 128 School Enrollment 46 9 12 37 21 125 Teen Birth Rate 47 9 19 34 14 123 Median Earning 48 22 15 25 11 121 Sales and Office Occupations 47 25 7 23 18 120 Avg. No. of Mentally Unhealthy Day 45 20 2 30 24 120 Children Under 6 Living in Poverty 47 16 9 23 23 118 Poverty Rate 48 15 17 21 16 117 Child Poverty 48 14 13 16 26 117 Adults 65 and Older Living in Poverty 47 12 33 8 15 115 Preschool Enrollment Ratio 48 12 8 14 33 114 Republican Win 2012 52 41 52 NA NA NA FIPS State Code 47 NA 32 NA 13 NA Based on this score, and correlation between the features, we can narrow down a group of features that are low-scoring and highly correlated–in this case, the group that we found consist mainly of poor-healthcare-quality and poverty-related features. We then proceed with principal component analysis (PCA) to derive a more compact set of features that can represent features in this group, since we do not want to get rid of this group altogether. The result is presented in table below. We select the top principal components that accounts for 90% of the total variance, i.e. PC1 to PC4. Although this technique is really powerful, its downside is that it does not provide an easy way to interpret the resulting principal components. To demonstrate our point that the components obtained from PCA do not give us a clear interpretation, let us consider Figure 4.3 (due to the limit of current visualization technique … we cannot really show all 4 components). Clearly, we can see that PC1, the most important component, is contributed by many original features so it is hard to tell what it really represent; as for PC2, perhaps we can conclude that it represents the group of health-related features). So the downside of using PCA is that we might lose some clarity in the interpretation of the model. Nevertheless, since our goal is to gain business understanding of the result, we need to be able to have a rough understanding of the chosen components. Refer to PC rotation table above, we conclude that PC1 represents a measure for general healthcare quality received by the low-income population since based on the rotation, it is contributed rather equally by poverty and poor healthcare. PC2 represents a measure for adult smoking-related health problem rate. PC3 is rather pure as it depends greatly on child poverty rate. PC4 is also contributed mainly by teen birth rate. Setting up 3D graphics has always been a struggle (the great value is that you can try to rotate and zoom). So if you cannot see this widget, you can enjoy the 2D version here Figure 4.3: Principal Components and Original Features The final step of this process is to remove the 10 low-scoring features and replace them with the 4 new principal components to obtain the Analytic Base Table, which can be viewed in Appendix A. 4.4 Modelling We will attempt to create models based on several different classifiers, including probability-based, rule-based, tree-based, instance-based, and linear-regression-based classifier. Following the convention, we will also include 1R as our base classifiers to set the lower bound for the performance of the other more sophisticated models (Witten et al. 2017). The detail for each classifier can be found in section 3 For each classifier, we will attempt to tune its parameters to obtain the best model. The criterion for “best” can potentially swing between accuracy, kappa or recall; we will go into further details about this decision when we discuss and evaluate the results. Since interpretability is the key to our study, we decide to classify the resulting models as either blackbox, which refers to models that we can hardly derive any meaning from its structure, such as SVM, or K-Nearest Neighbors, or whitebox, which refers to models that we can come up with some form of relationship between the original features by studying the structure of the model itself, such as decision tree, decision list (for rule-based classifiers’ models). Linear models are in a grey area here because based on the coefficients, we can somewhat tell the relationship between features but because its nature is closely-related to typical blackbox models produced by SVM or Neural Network classifiers, we also classify it as blackbox. In terms of pre-processing the data, Naive Bayes, the only probability-based classifier we use, suffers from skewed data distribution so discretization is required. Discretization is also a pre-requisite for 1R learner. As such, for these two classifiers, we tune the way data is discretized (supervised or equal-frequency binning; equal-interval binning does not help much with skewedness of data, so we leave it out) and the number of bins. For regression-based and instance-based classifiers, we have to turn nominal features into multiple binary features and turns the value into 0,1 for FALSE and TRUE. These classifiers are also known to be very susceptible to incoherent scale of data, so prior to this, we normalize all numeric features. SVM is an exception as it uses standardization as suggested by the specs of the algorithm. Also, for regression-based classifiers, we have to set a threshold for classification. Consequently, we tune this threshold for linear model though we keep the threshold at 0.5 for neural network as the classifier is known to be very robust and works well with this default threshold. Rule-based and tree-based classifiers are less demanding in terms of data pre-process. They are also non-parametric so they should work well with skewed data distribution. Another important issue to address is that we have an imbalanced data set: the number of counties won by Republican is roughly 6 times as many as that of Democratic. Since not all classifiers handle imbalanced class distribution well (He and Garcia 2009), we have to use under-sampling and over-sampling during training. But each method has its quirks. Under-sampling requires us to remove instance of the majority class to match the number of the minority class. Throwing away real data is never a great option. In turn, the size of the data set shrinks and this costs models learned using this training set more susceptible to the effect of the random split. So we have to increase the number of fold for cross-validation. Over-sampling, on the other hands, involves boostraping data with replacement to boost the number of minority class instances to match that of the majority class. Duplicating data is also not a great option neither since it randomly duplicates the minority data instances. This does not gain us any information, but also potentially leads to over-fitting (Ganganwar 2012). This also significantly increases the size of the data set, making computation slower and thus, we decide to reduce the number of fold in cross validation. Over-sampling has a slight advantage over undersampling in justifying for our reducing number of fold in cross validation in that it has bigger training set, and no instance is thrown away, as such, it might have slightly better performance. Nevertheless, for both under-sampling and over-sampling, we risk increasing the effect of outliers. Among the classifiers we use, rule-based classifiers are especially resilient towards imbalanced class distribution as they use separate-and-conquer approach, i.e. learns each instances that lead to one class value at a time. The SVM implementation we uses the engine of LIBSVM, which supports weighted SVM for imbalanced data. We will come back to imbalanced class issue during evaluation to decide which models is the best. Neural network and K-Nearest neighbor (both weighted and unweighted version) are among the most computationally expensive algorithms, as such, we run these classifiers using a large range for tuning parameter to probe for the best range to tune on during automated modelling. The results are presented below. Figure 4.4: Tuning for k in Unweighted K-Nearest Neighbor (top: under-sampling, bottom: over-sampling) For unweighted k-nearest neighbor, we skip the even values for k and look at the accuracy as well as the kappa value as we tune. Clearly, the regions where we get best results for both under- and over-sampling is 1 to 19 and 259 to 399. Figure 4.5: Tuning for k in Weighted K-Nearest Neighbor (top: under-sampling, bottom: over-sampling) For the weighted version, the optimal range for tuning is 1 to 10 and 20 to 70. For instance-based learning in general, we tend to avoid going for really high values of k. As illustrated, this often produce models with very low kappa; in a sense, it defeats the purpose the purpose of instance-based learning model if every time we classify a novel instance, we have to go through a huge number of existing instances. On the other hand, too small a number of k is not advisable as this might amplifies the effect of noise in our training set. For neural network, to save time, we test out several configurations for the hidden layers using just the undersampling training set. As we can see, once we attempt to go for more than 10 nodes per hidden layers, the result drop. We can see that the performance tends to peak when we use 2 layers with around 3 to 7 nodes for each. It is not a surprising result since 2 layers are often adequate to describe an arbitrarily complex relationship; whereas one layer is over-simplified and 3-layer configurations are too expensive. Figure 4.6: Tuning for hidden layer configuration for Neural Network The final range for tuning parameters used for each classifiers are listed below. 1R Binning method (oner.bin): either &quot;supervised&quot; or &quot;frequency&quot; Number of bins (oner.nbin): 10, 20, 30 ... 80 Naive Bayes Binning method (nb.bin): either &quot;supervised&quot; or &quot;frequency&quot; Number of bins (nb.nbin): 10, 20, 30 ... 80 Laplace estimator (nb.laplace): 0, 1 ... 4 PART Minimum number of objects per leaf (part.minLeaf): 1, 2 ... 15 Enable reduced-error pruning (part.REP): either TRUE or FALSE Confidence threshold for pruning (part.pruneConf): 0.25 RIPPER Minimum weight of instance within a split (jrip.minWeight): 0, 1 ... 20 C4.5 Minimum number of objects per leaf (part.minLeaf): 1, 2 ... 15 Enable reduced-error pruning (part.REP): either TRUE or FALSE Confidence threshold for pruning (part.pruneConf): 0.25 CART Complexity parameter (rpart.cp): 0.00, 0.05 ... 1.00 Linear Model Decision threshold (lm.threshold): 0.30, 0.35 ... 0.80 Supporting Vector Machine (SVM) Kernel type (svm.kernel): &quot;linear&quot;, &quot;polynomial&quot;, &quot;radial&quot; or &quot;sigmoid&quot; Cost of constraints violation (svm.cost): 0.1, 0.2 ... 1.3 Neural Network Algorithm (neuralnet.algorithm): either &quot;rprop+&quot; or &quot;rprop-&quot; Hidden layer(s) configuration (neuralnet.hidden): (2,1), (3,2), (4,3), (5,4), (5,2), or (7,5) Unweighted K-Nearest Neighbor Number of nearest neighbor (knn.k): 1, 3 ... 19 and 259, 263 ... 399 Weighted K-Nearest Neighbor Number of nearest neighbor (kknn.k): 1, 2 ... 10 and 20, 21 ... 70 4.5 Result and Evaluation Our main objective is to see how demographic factors help determine the winner at each county; however, as mentioned, we are also interested in understanding how past year result, i.e. historical factor and the state to which a county belongs, i.e. geographic factor affect the result. As such, we proceed the modelling phase independently for three types of set of feature. For brevity purpose, we denote these types as: hist which consists of all features; geog which excludes the 2012 result; and demo which only includes the demographic features. In terms of performance, similar to what we have talked about in feature selection, we look for models with highest accuracy and kappa. Figure 4.7: Accuracy and Kappa by Type Clearly, here we can see the superior performance of the hist model, i.e. which includes the 2012 result. What interests us more is the spread in performance of the models learned using geog data set. Perhaps, our previous explanation might fit well in here. We argue that state is a complex composite indicator that potentially encodes unique information on various demographic aspects. Including the state might help to improve the result of states that we refer to as homogenous, in other words, states where most counties are similar in demographic statistics and election trend. For swing-states or states where counties may vary vastly in demographic data such as California, including state in the learning process may actually hamper and obfuscate the learning algorithms. For this very reason, our interest shifts more towards the demo set, which comprises purely demographic features. Now, let us consider the effect of under-sampling and over-sampling on the performance of various models. Since we can see that most models perform relatively well in terms of accuracy (&gt; 0.75), we will use Kappa as the metric to measure of performance. Figure 4.8: Kappa by Sampling Method Here we can see the unique effect that each type of sampling acts on the models. For under-sampling, since many instances are removed, the size of the data set shrinks significantly, this aggravates models’ performance. For over-sampling, since there are a larger data set, the model tends to performs better and the spread of models’ performance is usually the smallest. When no sampling method is applied, models performance spreads out in a wide range as there are classifiers that works well with imbalanced data and there are classifiers that are hurt by this imbalance, such as tree-based and k-nearest neighbor. Nevertheless, top-performing models are often those trained with unsampled data set. This substantiates our point early about how both under-sampling and over-sampling potentially amplify the effect of noises in the data set. Another interesting point to note is the interaction of the data set type and the different sampling methods. As can be seen, for no-sampling, there is a wide spread of kappa values for models learning the hist and geog data set. This is totally expected as with the presence of strong predictors like 2012 result and state, random predictors would perform relatively better. The reason why we pay extra attention to the sampling method is already explained in the previous section. To see how these actually affect the learned models, we should look at the recall of the minority class (Democratic as the winner) as well as the prediction rate of the majority class (Republican as the winner). Figure 4.9: Recall for Minorty Class and Precision for Majority Class by Sampling Method As expected, models learned using unsampled training set has the tendency to favors the majority class, as such, a large number of instances that actually belongs to the minority class are predicted to belong to the majority class. This shows us that during evaluation, we have to pay close attention to these 2 metrics beside kappa. Now let us look at the general performance of each classifiers over all data set type and sampling method. Figure 4.10: Kappa by Classifier And of course, because God is in the detail, we present the performance of each classifiers by 3 different data set types and sampling methods below. Figure 4.11: Kappa by Classifier We can see that the top performers are linear-regression-based, rule-based, and tree-based classifiers. The only exception is the tree-based classifier CART, whose performance spreads in a wildly wide range. We think that this is mainly due to the fact that we allow the complexity parameter, the only tuning parameter for CART, to vary between 2 extremes, i.e. 0 to 1, which corresponds with no pruning and pruning everything. Nevertheless, its performance in most case are not really impressive anyway. Instance-based classifiers’ performance barely passes those of base classifiers 1R and the simple Naive Bayes classifiers. Also, we are surprised at first to how neural network did not outperform simpler classifiers like linear regression or SVM; perhaps the underlying function is actually simpler than we thought and that using neural network with 2 layers actually hampers the performance of the resulting models by taking bias against the intrinsic simplicity of the underlying function. But since we are dealing with an ill-posed problem, we cannot say anything too certain about this. The most consistent top-performers are linear model and SVM. However, these models also faces the same problem as CART where the performance result spreads in a wide range. We think that a similar explanation can be used in this case. For linear model, we tune the threshold of classification over the range from 0.3 to 0.8; and for SVM, we tune the cost of constrain violation over an wide range from 0.1 to 1.3–both of these directly affect how these models classify test instances and inherently, their performance. After this, we identify the best blackbox and whitebox models for each data set type and apply our understanding of the algorithms and sampling methods to choose the best models for each type. First, we will consider the hist data set. Note that in this table, “Precision*” represents precision rate for majority class and “Recall*” represents the recall for minority class. In the context of this analysis, we will use recall and precision when talking about these. Model 5 is the best blackbox model as it ranks highest in Kappa and has relatively high precision and recall rate. This model is obtained by using SVM on over-sampled training set with linear kernel and cost of 0.4. Model 4 is the best whitebox model. It is obtained by using RIPPER (RWeka::JRip implementation) on unsampled training set with minimum weight of instance within a split set to 10. RIPPER, as noted, works well with imbalanced class distribution, so we have no problem here picking it as the best model. Evidently the recall and precision rate for this model is also relatively high. The set of rules produced by it is presented below. 1. (Republican Win 2012 = FALSE) and (Winter Avg. Temperature &gt;= 29.536101) and (Median Age &lt;= 38.9) =&gt; Republican Win 2016=FALSE (233.0/12.0) 2. (Republican Win 2012 = FALSE) and (Adult Obesity Rate &lt;= 0.276) and (At Least Bachelor Degree &gt;= 28.2) =&gt; Republican Win 2016=FALSE (114.0/7.0) 3. (Republican Win 2012 = FALSE) and (White not Latino Population &lt;= 49.6) =&gt; Republican Win 2016=FALSE (87.0/12.0) 4. (Republican Win 2012 = FALSE) and (Farming Fishing and Forestry Occupations &lt;= 0.4) and (Median Age &lt;= 39) and (Median Earning &gt;= 26201) =&gt; Republican Win 2016=FALSE (12.0/1.0) 5. (Republican Win 2012 = FALSE) and (At Least Bachelor Degree &gt;= 19.8) and (African American Population &lt;= 0.6) and (HIV Prevalence Rate &gt;= 58.288) and (Population Density &gt;= 7.541532) =&gt; Republican Win 2016=FALSE (17.0/4.0) 6. (Republican Win 2012 = FALSE) and (Other races Population &gt;= 2.1) and (Adult Obesity Rate &lt;= 0.295) =&gt; Republican Win 2016=FALSE (24.0/7.0) =&gt; Republican Win 2016=TRUE (2623.0/41.0) As we can see, all of the rules are related to the 2012 result. To us, this might provide some insights, yet not particularly interesting. We will thus, move on the geog data set. Model 3 is the best blackbox model as it ranks highest in Kappa and has relatively high precision and recall rate. This model is obtained by using SVM on unsampled training set with radial kernel and cost of 1.1. In terms of Kappa and accuracy, we would choose model 4 over model 6 as the best white box model, but model 4 is obtained using C4.5 decision-tree algorithm (RWeka::J48 implementation) on unsampled training set, which accounts for the relatively lower recall rate at 0.75. As such, we consider model 6 the best whitebox model for the geog data set. This model is obtained by using RIPPER on over-sampled training set with minimum weight of instance within a split set to 8. The set of rules learned by the model is presented below. 1. (White not Latino Population &gt;= 55.5) and (Graduate Degree &lt;= 8) and (At Least Bachelor Degree &lt;= 18.4) =&gt; Republican Win 2016=TRUE (1558.0/6.0) 2. (White not Latino Population &gt;= 55.5) and (Graduate Degree &lt;= 7.1) and (Winter Avg. Temperature &gt;= 28.25) =&gt; Republican Win 2016=TRUE (194.0/0.0) 3. (HIV Prevalence Rate &lt;= 105.277) and (Graduate Degree &lt;= 10.2) and (Children in Single Parent Households Rate &lt;= 0.308) and (Infant Mortality &gt;= 5.8) =&gt; Republican Win 2016=TRUE (184.0/0.0) 4. (HIV Prevalence Rate &lt;= 191.294) and (Adult Obesity Rate &gt;= 0.271) and (White not Latino Population &gt;= 35.3) and (Age Dependency Ratio &gt;= 66.4) and (Median Age &lt;= 50.1) and (Winter Avg. Precipitation &lt;= 11.71) =&gt; Republican Win 2016=TRUE (142.0/0.0) 5. (White not Latino Population &gt;= 64.3) and (At Least Bachelor Degree &lt;= 27.6) and (Low Birth Weight Rate &gt;= 0.067619) and (Winter Avg. Temperature &gt;= 31.785417) and (Asian American Population &lt;= 4) =&gt; Republican Win 2016=TRUE (114.0/0.0) 6. (HIV Prevalence Rate &lt;= 152.548) and (Gini Coefficient &lt;= 0.434) and (Other races Population &lt;= 1.4) and (Injury Death Rate &lt;= 57.5) and (Voting Power &lt;= 0.415249) =&gt; Republican Win 2016=TRUE (47.0/0.0) 7. (White not Latino Population &gt;= 51.1) and (Graduate Degree &lt;= 9.4) and (Other races Population &lt;= 1.8) and (Winter Avg. Temperature &gt;= 23.739286) and (Population Density &lt;= 19.864428) =&gt; Republican Win 2016=TRUE (54.0/0.0) 8. (White not Latino Population &gt;= 73.8) and (Graduate Degree &lt;= 12.4) and (Voting Participation Rate &lt;= 54.80236) and (Diabetes Rate &gt;= 0.087) =&gt; Republican Win 2016=TRUE (35.0/0.0) 9. (HIV Prevalence Rate &lt;= 191.294) and (Adult Obesity Rate &gt;= 0.243) and (Service Occupations &lt;= 21) and (Asian American Population &lt;= 2.7) and 10. (Children in Single Parent Households Rate &lt;= 0.286) and (Infant Mortality &lt;= 5.7) =&gt; Republican Win 2016=TRUE (42.0/0.0) 11. (White not Latino Population &gt;= 50.2) and (Graduate Degree &lt;= 12.3) and (Voting Participation Rate &lt;= 53.47034) and (Unemployment Rate &lt;= 0.063) =&gt; Republican Win 2016=TRUE (25.0/0.0) 12. (White not Latino Population &gt;= 67.8) and (Age Dependency Ratio &gt;= 59.2) and ([PC] Child Poverty Rate &lt;= -0.147481) and (Native American Population &gt;= 0.2) =&gt; Republican Win 2016=TRUE (23.0/0.0) 13. (Age Dependency Ratio &gt;= 61.3) and (White not Latino Population &gt;= 47.3) and ([PC] Teen Birth Rate &gt;= 0.171219) and (Other races Population &lt;= 1.1) =&gt; Republican Win 2016=TRUE (20.0/0.0) 14. (HIV Prevalence Rate &lt;= 194.363) and (Adult Obesity Rate &gt;= 0.267) and (Service Occupations &lt;= 17.6) and (Winter Avg. Precipitation &lt;= 3.125) =&gt; Republican Win 2016=TRUE (18.0/0.0) 15. (White not Latino Population &gt;= 53.2) and (Adult Obesity Rate &gt;= 0.271) and (Winter Avg. Temperature &gt;= 43.8) and (Sex Ratio &lt;= 94.6) =&gt; Republican Win 2016=TRUE (15.0/3.0) 16. (White not Latino Population &gt;= 84.8) and (Homogeneity Index &lt;= 0.82847) and (Production Transportation and Material moving Occupations &gt;= 11.3) and (Other races Population &lt;= 1.8) =&gt; Republican Win 2016=TRUE (14.0/0.0) 17. (HIV Prevalence Rate &lt;= 193.533) and (Infant Mortality &gt;= 6.639923) and (Service Occupations &lt;= 20.9) and (Unemployment Rate &lt;= 0.062) and 18. ([PC] Child Poverty Rate &lt;= 1.224397) and (Crime Rate &lt;= 343.11) =&gt; Republican Win 2016=TRUE (18.0/0.0) 19. (White not Latino Population &gt;= 75.7) and (Graduate Degree &lt;= 10.2) and (Winter Avg. Temperature &gt;= 22.99) and (Gini Coefficient &lt;= 0.434) and 20. (Asian American Population &lt;= 1.6) and (Sex Ratio &lt;= 102.3) =&gt; Republican Win 2016=TRUE (13.0/0.0) 21. (Gini Coefficient &lt;= 0.443) and (Preschool Enrollment Ratio &lt;= 38.6) and (Median Earning &lt;= 24755) and (Diabetes Rate &lt;= 0.127) and (Uninsured Rate &gt;= 0.143) =&gt; Republican Win 2016=TRUE (16.0/0.0) 22. (White not Latino Population &gt;= 58.1) and (Winter Avg. Temperature &gt;= 39.1) and (Sales and Office Occupations &gt;= 26.2) and (Children in Single Parent Households Rate &lt;= 0.355) =&gt; Republican Win 2016=TRUE (13.0/0.0) 23. (Sex Ratio &gt;= 102.5) and (Preschool Enrollment Ratio &lt;= 49.8) and (White not Latino Population &gt;= 43.3) and (White not Latino Population &lt;= 79.7) and (Voting Participation Rate &lt;= 69.594874) =&gt; Republican Win 2016=TRUE (14.0/0.0) =&gt; Republican Win 2016=FALSE (2691.0/75.0) Here, we can see a much more interesting set of rules in the sense that the state does not actually appear in any rule. We will definitely talk more about this in the discussion section. For now, we will move on to our main objective, i.e. the demo data set. Again, SVM dominates blackbox type models. Model 3 is the best blackbox model, which tops all 4 metrics. This model is obtained by using SVM on unsampled training set with radial kernel and cost of 1.1, very similar to the one obtained for geog data set. As for the best whitebox model, we have a heated competition between model 4 and 6. Model 4 has slightly higher Kappa (0.74) but lower recall rate (0.77) whereas model 6 does really well in term of recall (0.81) but slightly lower Kappa (0.71). Model 4 is obtained from using PART algorithm (RWeka::PART) on unsampled training set, with minimum number of nodes in leaf set to 8 and pruning confidence set to 25%. Model 6 is also obtained using PART but trained on over-sampled data set, with reduced-error pruning enabled. Since PART first builds partial (C4.5) decision tree on the current set of instances and makes the “best” leaf into a rule then discards the instances covered by the rule, it is at a slight disadvantage while working with imbalanced class distribution. As such, in this case, we prefer model 6 to model 4. The set of rules learned by the model is presented below. 1. (White not Latino Population &lt;= 50.9) and ([PC] Teen Birth Rate &lt;= 0.605307) and (Native American Population &lt;= 22.4) and (School Enrollment &gt; 69.2) and (HIV Prevalence Rate &gt; 225.949) =&gt; Republican Win 2016=FALSE (469.0/1.0) 2. (At Least Bachelor Degree &gt; 26.5) and (Children in Single Parent Households Rate &gt; 0.17) and (Winter Avg. Precipitation &gt; 2.27) and ([PC] Child Poverty Rate &gt; -0.060527) and (Age Dependency Ratio &lt;= 65.6) and ([PC] Adult Smoking Related Health Problem Rate &lt;= 1.006921) and (Adult Obesity Rate &lt;= 0.242) =&gt; Republican Win 2016=FALSE (277.0/3.0) 3. (Population Density &gt; 145.425672) and (White not Latino Population &lt;= 73.7) and (Winter Avg. Temperature &lt;= 50.45) and (White not Latino Population &lt;= 59.1) =&gt; Republican Win 2016=FALSE (130.0) 4. (White not Latino Population &lt;= 38.8) and (Diabetes Rate &gt; 0.131) =&gt; Republican Win 2016=FALSE (208.0/6.0) 5. (Graduate Degree &lt;= 8) and (African American Population &lt;= 46.9) and (Homogeneity Index &gt; 0.405302) and (Graduate Degree &lt;= 6.9) and (White not Latino Population &gt; 41.9) and (Adult Obesity Rate &gt; 0.283) and (Sexually Transmitted Infections Rate &lt;= 590.2) =&gt; Republican Win 2016=TRUE (1083.0/2.0) 6. (Graduate Degree &gt; 9.9) and (Population Density &gt; 187.742929) and (Sales) and (Office Occupations &lt;= 27.8) and (Sexually Transmitted Infections Rate &gt; 245.4) =&gt; Republican Win 2016=FALSE (197.0/3.0) 7. (Winter Avg. Precipitation &gt; 19.11) and (Unemployment Rate &lt;= 0.09) and (Low Birth Weight Rate &gt; 0.05) =&gt; Republican Win 2016=FALSE (64.0/5.0) 8. (African American Population &lt;= 50.7) and (Graduate Degree &gt; 12.6) and (Crime Rate &gt; 73.8) and (Homicide Rate &lt;= 5.156087) and (Voting Participation Rate &gt; 49.777857) and (Farming Fishing) and (Forestry Occupations &gt; 0.4) =&gt; Republican Win 2016=FALSE (52.0) 9. (African American Population &lt;= 50.7) and (White not Latino Population &gt; 33.7) and (Asian American Population &gt; 0.2) and (Other races Population &lt;= 1.1) and (Construction Extraction Maintenance) and (Repair Occupations &gt; 7) and (Low Birth Weight Rate &gt; 0.05) =&gt; Republican Win 2016=TRUE (130.0) 10. (Unemployment Rate &lt;= 0.033) and (Homicide Rate &gt; 1.9) =&gt; Republican Win 2016=TRUE (40.0) 11. (Asian American Population &lt;= 0.2) and (African American Population &lt;= 51.5) =&gt; Republican Win 2016=TRUE (72.0) 12. (African American Population &lt;= 40.8) and (Production Transportation) and (Material moving Occupations &gt; 18.6) =&gt; Republican Win 2016=TRUE (49.0/2.0) 13. (Adult Obesity Rate &lt;= 0.381) and (African American Population &gt; 26.1) and (Uninsured Rate &lt;= 0.211) and (Production Transportation) and (Material moving Occupations &lt;= 14.7) =&gt; Republican Win 2016=FALSE (47.0/1.0) 14. (African American Population &gt; 46.6) =&gt; Republican Win 2016=FALSE (32.0) 15. (Winter Avg. Temperature &lt;= 23.9) and (Age Dependency Ratio &gt; 69.3) and (Latino Population &gt; 0.7) =&gt; Republican Win 2016=TRUE (13.0) 16. (Winter Avg. Temperature &lt;= 23.9) and (Children in Single Parent Households Rate &gt; 0.17) and (Homicide Rate &lt;= 2.701087) and (Injury Death Rate &gt; 52.4) and (Diabetes Rate &lt;= 0.095) =&gt; Republican Win 2016=FALSE (77.0) 17. (White not Latino Population &lt;= 34.1) and (Adult Obesity Rate &lt;= 0.294) =&gt; Republican Win 2016=FALSE (24.0) 18. (At Least Bachelor Degree &gt; 39.6) and (Children in Single Parent Households Rate &gt; 0.15) and (HIV Prevalence Rate &gt; 62.041) and (Sales) and (Office Occupations &lt;= 26.7) and (African American Population &lt;= 12.6) =&gt; Republican Win 2016=FALSE (35.0) 19. (Latino Population &lt;= 38.3) and (Preschool Enrollment Ratio &gt; 33.9) and (Winter Avg. Temperature &gt; 26.814286) and (Asian American Population &lt;= 3) and (Children in Single Parent Households Rate &lt;= 0.455) and (School Enrollment &lt;= 83.8) and (Preschool Enrollment Ratio &lt;= 52.7) and (Low Birth Weight Rate &gt; 0.065) =&gt; Republican Win 2016=TRUE (124.0) 20. (Children in Single Parent Households Rate &gt; 0.197) and (Median Age &gt; 47) and (Latino Population &gt; 0.9) =&gt; Republican Win 2016=TRUE (32.0) 21. (Children in Single Parent Households Rate &gt; 0.197) and (Adult Obesity Rate &gt; 0.212) and (Preschool Enrollment Ratio &lt;= 33.1) and (Other races Population &gt; 0.6) =&gt; Republican Win 2016=TRUE (30.0) 22. (Children in Single Parent Households Rate &lt;= 0.197) =&gt; Republican Win 2016=TRUE (28.0) 23. (Voting Power &lt;= 0.014387) and (African American Population &lt;= 2.6) =&gt; Republican Win 2016=FALSE (17.0) 24. (Diabetes Rate &lt;= 0.088) and (Asian American Population &lt;= 2.2) =&gt; Republican Win 2016=TRUE (33.0) 25. ([PC] Child Poverty Rate &lt;= -0.105728) and (Latino Population &lt;= 55.7) =&gt; Republican Win 2016=TRUE (32.0/3.0) 26. (Infant Mortality &lt;= 4.3) =&gt; Republican Win 2016=FALSE (16.0) 27. (Voting Participation Rate &gt; 51.618184) and (Winter Avg. Precipitation &gt; 12.14) and (Low Birth Weight Rate &gt; 0.05) =&gt; Republican Win 2016=TRUE (9.0) 28. (Voting Participation Rate &gt; 51.618184) and (Homicide Rate &gt; 5.719412) =&gt; Republican Win 2016=FALSE (29.0) 29. (Age Dependency Ratio &gt; 65.9) and (Voting Participation Rate &lt;= 64.880023) =&gt; Republican Win 2016=TRUE (14.0) 30. (Production Transportation) and (Material moving Occupations &lt;= 8.7) and (Construction Extraction Maintenance) and (Repair Occupations &gt; 5.7) =&gt; Republican Win 2016=TRUE (9.0) 31. ([PC] Child Poverty Rate &lt;= -0.065225) =&gt; Republican Win 2016=FALSE (17.0) 32. (Preschool Enrollment Ratio &gt; 53.6) and (Asian American Population &lt;= 3.8) =&gt; Republican Win 2016=TRUE (13.0) 33. (Other races Population &gt; 2) and (Homicide Rate &gt; 2.4) and ([PC] Adult Smoking Related Health Problem Rate &gt; -0.235165) =&gt; Republican Win 2016=FALSE (41.0/1.0) 34. (Sex Ratio &lt;= 96) and (Gini Coefficient &gt; 0.421) =&gt; Republican Win 2016=FALSE (18.0) 35. (Service Occupations &lt;= 21.3) and (Injury Death Rate &gt; 41.7) and (African American Population &gt; 0.9) =&gt; Republican Win 2016=TRUE (23.0) 36. (Preschool Enrollment Ratio &gt; 43.2) =&gt; Republican Win 2016=FALSE (12.0) 37. (School Enrollment &lt;= 77.3) =&gt; Republican Win 2016=TRUE (3.0) =&gt; Republican Win 2016=FALSE (1.0) 4.6 Discussion In this section, we will attempt to interpret some of the rules learned by the whitebox models. Note that to better our understanding, we leave the data unnormalized when we build the model so that all the split points used in each antecedent are of the same unit as that of the original features. Arguably, this might affect the model constructed but generally this should not affect greatly since rule-based classifiers treat both normalized and un-normalized features alike. We also recognize that based on its mechanism, RIPPER will choose the majority class as the default and build rules to cover instances of the minority classes, but in this case, the training data was oversampled so there is no real major or minor classes here. As such, the choice is randomized every time. To make this result reproducible (as well as other procedures reproducible), we fix a seed at the beginning of the script. This also helps PART since we found that this particular algorithm actually produces slightly different set of rules each time we run it. Also note that since we used cross-validation for model-tuning step, the models that we build in the end use all of the data set as its training set, and the assumed performance is the performance as dictated by the cross-validation process. Since both RIPPER and PART produce a decision list, we will only care about rules with highest coverage, in this case, we set a threshold to approximately 50 instances, i.e. rules covering less than this threshold will not be considered in this discussion. Nevertheless, some rule with high coverage appear really obscure to us. For example, for geog model rule number 3–(HIV Prevalence Rate &lt;= 105.277) and (Graduate Degree &lt;= 10.2) and (Children in Single Parent Households Rate &lt;= 0.308) and (Infant Mortality &gt;= 5.8) =&gt; Republican Win 2016=TRUE (184.0/0.0). For graduate degree, from the section @(dqr-exploration), we can see that 10.2 is beyond the upper quartile range. As such, this is a high graduate degree rate, but the direction of the arrow is less than. So how should we interpret this antecedent? Should we say that counties where a lot of people have graduate degree favor Republican. But this is erroneous, because the sign suggests that counties with extremely small number of people with graduate degree are also considered. As such, rules with this kind of antecedents are not really useful to us. Other than that problem, we are quiet comfortable with interpreting some of the top rules. For geog data set, the major rules learned are presented below: The number of the rule corresponds to the number of the raw rule extracted from the model. Counties where more than half of the population is white and have relatively low level of education achievement, such as bachelor/graduate degree favor Republican Counties where more than half of the population is white and have higher winter temperature favor Republican Counties where HIV presents at moderate level and have more than third of the population white and high age-dependency ratio with relatively young population and extremely dry weather favor Republican Counties with majority of the population white, low education achievement rate, a small portion of Asian American and relatively warm winter favor Republican As for the the main data set demo, some of the interpretation we made are presented below. Counties where less than half of the population is white, with relatively low teen birth rate, less than a quarter of population as native American, relatively high school enrollment and relatively high HIV prevalence rate favor Democratic Counties with high population density and where less than less than half of the population is white favor Democratic Counties with less than one third of the population as white and relatively high diabetes rate favor Democratic Counties where less than half of the population is African American and more than half is white and relatively high adult obesity rate and low graduate degree achievement rate as well as high sexually transmitted infections rate favor Republican Counties with relatively high rate of graduate degree achievement, highly-populated and less than a quarter of the population working in sales and office occupations but high sexually transmitted infections rate favor Democratic Counties with very small portion of Asian American and less than half of the population as African American favor Republican Counties with less than third of the population as Hispanic, warmer winter, pre-school enrollment ratio within 30% and 50%, high children in single parent household rate and high low birth weight rate favor Republican Counties where infant mortality rate is low favor Democratic Admittedly, even when we spread out these rules in front of us, they are still rather cryptic and convoluted to a certain extent. To simplify this, we think it is best we look for certain patterns in this rule-set and give some advice to each party on how they might potentially improve their chance of winning. Counties which support Republican tend to have a large portion of the population as white, warmer weather and/or relatively low level of education achievement. As such, Democratic party might want to focus their campaign more towards these areas. On the other hand, Democratic party often wins counties with higher race homogeneity index, i.e. a smaller portion of the population as white. Also, highly-populated counties tend to favor Democratic party. This result is not surprising as we can see how Democratic party managed to win some past election despite winning significantly lower number of counties. We also observe an interesting pattern that counties with lower quality of healthcare favor Democratic party. Perhaps, Republican can adjust their healthcare policies to make them appear more attractive to these counties. 4.7 Conclusion Through this study, we have learned some interesting points about the 2016 election result, such as how counties with warmer weather tend to support Republican or counties with lower healthcare quality favor Democratic. We find these findings attractive in its own way as we obtain them only though numbers and figures, without politics, without speeches, without literary devices. Maybe in the future, some socioeconomists might actually figure out how warmer weather works out well for Republican in a convincing way. But what we have now to back what we find are just numbers. Numbers are cold though, and despite their seemingly innocuous look, they conceal highly sophisticated truth. Or perhaps, can it turn out to be really simple? We are never sure of anything, in fact we can only say we are x% sure of something, because every problems we are trying to solve using data mining are ill-posed problems. It can get even worse when we realize that our best models perform equally well as a 1R model using just one feature, such as the 2012 result, does. Nevertheless, due to the relatively high performance, at least we can say we are rather confident that there are some sort of underlying function between the descriptive features and the target. At some point, we thought that data mining is THE unbiased approach to study anything. But we were wrong, classifiers are actually pretty biased: some prefer simpler structure using MDL principle, some prefer one metric over another, etc. Bias is what fuels the decision power of classifiers. However, data mining is powerful in that it does not let itself confused by meaning of the features. Humans like us are too quick to pick up patterns in terms of meaning and are quick to fall into the bog of confirmation bias. Numbers are cold and machine treat them coldly too. So definitely, there is something neat about this cold way of learning; and yet again, the hardest part is to look for the perfect balance between the two forces–human and machine. But trying to figure that, unfortunately, is also an ill-posed problem itself. 4.8 Further Thoughts In terms of improvement, we really want to have a better understanding of factor analysis technique since that will help us derive more meaningful features during feature selection phase. We also feel that we have to approach data collection phase with better business understanding, as of now, we have categories of features with vastly different granularity. Speaking of categories, we spent a considerable amount of time on trying to do feature selection differently. When we try to use exhaustive search to find the best attribute, this took us too long so we thought, we might instead be able to figure out what groups of features, i.e. categories, often go well with one another in formulating the best models. In particular, we find subsets of categories that contribute to top-performing models; then we conduct association rule mining on these subsets. Below, we present the results of the best rules based on support, confidence and lift. Figure 4.12: Top Association by Support Figure 4.13: Top Association by Confidence Figure 4.14: Top Association by Lift By support, we can tell that race, healthcare, weather, education, demographic and election are categories that often appear in the subset of categories that make up good models. By confidence, we can see that other categories often couples with race in making top rules. And last but not least, by lift, we actually see the group of categories that form strong models. Race actually are not close to any particular groups, though alone it can be a good predictor factor. The group of weather, crime, finance and demographic (general demographic information such as sex ratio, age-dependency ratio, etc.) actually form a strong group. Based on these observation, we can then try to subset our feature space into several different sets and learn models using each of them accordingly. That is an improvement that we will attempt in the future. Last but not least, although, we can roughly say that warmer weather somehow correlates to higher win rate for Republican, we never actually be able to prove that, even using just number. Warmer weather still need to couple with another antecedent to make up a rule. One potential direction for further research is to isolate weather out and to study in-depth the relationship between weather and the election result. This also applies to healthcare quality. References "],
["appendix-data.html", "A Dataset Pre-Processed Data Prepared Data (Analytic Base Table)", " A Dataset Below are the snapshots of the data set at the two major phases, namely pre-processed and prepared. Again, we couple the data tables along with their choropleth since we believe that choropleth is a really good way to visualize data quickly. The raw data sets and intermediate-phase data sets can be found in the data folder. See Appendix B for more information on the data folder. Pre-Processed Data Figure A.1: Preprocessed Data set Prepared Data (Analytic Base Table) Figure A.2: Prepared Data (ABT) "],
["appendix-dir.html", "B Directories", " B Directories The base folder can be accessed at locally or via web browser using this link. The directory tree is presented below. . | |-- keynote |-- presentation // visualization support for the keynote |-- document // the report and article |-- code | | | |-- apps | |-- report | |-- slide // for the presentation | |-- utils | |-- data | | | |-- misc | |-- original | |-- pre_processed | |-- prepared | |-- selected | |-- result | | | |-- data_analyzed | | | | | |-- histograms | | |-- plots | | | |-- data_feature_ranked | | | | | |-- formula | | | |-- data_pre_processed | |-- data_prepared | |-- data_selected | | | | | |-- plots | | | |-- model_analyzed | | | | | |-- plots | | | |-- model_probed | | | | | |-- plots | | | |-- model_tuned | |-- backup // store previous results "],
["appendix-code.html", "C Code Usage Notice Structure Procedures License", " C Code Usage To start the local servers serving the keynote, report, and Shiny apps, run the bash script in the code folder. To adjust the ports used for each app, see the configs.R file. ./start.sh To make a full run of the project, uses the automated R script. Refer to the configs file to see available settings. Beware that the script might take a long time to run (7 ~ 8 hours on my machine). First start with opening R shell (you can also run from bash using R CMD BATCH). R Then source the automated script source(&quot;auto.R&quot;) Potentially, while running either of these task, you will need to install R libraries and packages. To do this, you can preemptively run the utils.R and the app.R scripts. It is likely that you will be prompted for packages installations. The utils.R will install packages used by main body of the scripts source(&quot;utils.R&quot;) app.R will install packages used for the Shiny apps. source(&quot;app.R&quot;) Notice To make sure that the result that we obtained are reproducible, we set seed for each script files where randomization is needed. This can be turned off using the config file. Notice that we use rgl and ShinyRGL packages for some of the visualizations. This requires XQuartz for Mac OSX and X11 for Linux. We have experienced inconsistent results on different Linux distros. As pointed out in this thread, you might need to have OpenGL installed properly and XVFB running since we are rendering the plots in the background using headless server mode. We recommend using Chrome to view the report for smoother animations and visual effects. Also, since Keynote works best on Mac OSX platform, viewing the keynote on Mac is preferred; but because the keynote has been converted to HTML, it should work well at least 90% of the time on other platforms. The default addresses for these documents are Keynote: http://localhost:2301 Report: http://localhost:2302 Keynote Visualization Support: http://localhost:2303 Structure All of the code used for this project lies in the code folder (see Appendix B). The names of the most of the scripts are self-explanatory since they are named after the phase for which they are responsible, for instance, data_feature_rank.R is for scoring features based on various feature selection techniques. Other scripts, such as utils.R, app.R, etc., are used for general purposes. utils.R is the heart of the project, it is the loader for the various 3rd libraries used by the scripts as well as several useful libraries we wrote ourselves. It masks the content of utils folder. configs.R acts like a preference section for the project. auto.R is an automated script to run the scripts in sequence to produce all the results and documents (except for the keynote). misc.R stores miscellaneous functions. app.R, start.R, and end.R are used to set up both local Shiny servers hosting various apps used in the documents and local servers that serve the keynote and the report (to specify the port used for these servers, refer to configs.R). The keynote was made using Apple’s Keynote. The report is made using the bookdown by Yihui Xie, and the visualization support for the keynote was made using Rstudio’s Rmarkdown and RevealJS. Procedures As mentioned throughout the section 1, 2, and 3. We mark certain parts of the code with labels to make code tracing and understanding easier. Following, we present a table of where the code chunks are located for each label. Label File Phase Source Code TR001 data_pre_process.R pre-process View TR002 data_pre_process.R pre-process View TR003 data_pre_process.R pre-process View TR004 data_pre_process.R pre-process View TR005 data_pre_process.R pre-process View TR006 data_pre_process.R pre-process View TR007 data_pre_process.R pre-process View TR008 data_pre_process.R pre-process View TR009 data_pre_process.R pre-process View TR010 data_pre_process.R pre-process View ED001 data_pre_process.R pre-process View ED002 data_pre_process.R pre-process View AD001 data_pre_process.R pre-process View AD002 data_pre_process.R pre-process View AD003 data_pre_process.R pre-process View AD004 data_pre_process.R pre-process View AD005 data_pre_process.R pre-process View AD006 data_pre_process.R pre-process View AD007 data_pre_process.R pre-process View AD008 data_pre_process.R pre-process View UP001 data_pre_process.R pre-process View UP002 data_pre_process.R pre-process View HO001 data_prepare.R prepare View HO002 data_prepare.R prepare View TR011 data_prepare.R prepare View TR012 data_prepare.R prepare View TR013 data_prepare.R prepare View TR014 data_prepare.R prepare View HM001 data_prepare.R prepare View HM002 data_prepare.R prepare View HM003 data_prepare.R prepare View HM004 data_prepare.R prepare View HM005 data_prepare.R prepare View FS001 data_feature_rank.R prepare View FS002 data_feature_rank.R prepare View FS003 data_feature_rank.R prepare View FS004 data_feature_rank.R prepare View FS005 data_feature_rank.R prepare View FS006 data_select.R prepare View MD001 model_probe.R modelling View MD002 model_probe.R modelling View MD003 model_probe.R modelling View MD004 model_probe.R modelling View MD005 model_tune.R modelling View MD006 model_tune.R modelling View License Based on our limited understanding of software licensing, we believe that this program should be licensed under the GNU General Public License 3.0. "],
["references.html", "References", " References "]
]
