# A Brief Note on Modelling {#md}




## Overview

In this section, we will briefly go through our selection of classifiers. More than just that, for each model, we want to investigate which parameters we can tune to optimize the performance; and for each parameter, we want to obtain a reasonable range of values so we can automate the tuning process. Also, each model has its odds and ends. At least, since we have dealt with all the missing values in our data in section \@ref(dqp), we just need to care about the type of the descriptive features and the target (nominal or continuous). Besides, each modal potentially requires different data pre-process procedures. Also, for each tune, we run cross validation to minimize the effect of random splits. Last but not least, we take the huge difference between the number of counties won by the Republican party and that of the Democratic party into account, and so we can either under-sample or over-sample the training set. 

In the next part, we list out the main procedures we use in the modelling process. We use the term "probing" to describe the procedure in which we identify the range of values for the parameters of computationally expensive classifiers, such as K-Nearest Neighbor and Neural Network. Essentially, we perform a preliminary tuning for these classifiers. For the data set used, We feel that we should not include the 2012 results as well as the state FIPS code to avoid any effect that these two features might contribute to the performance of the data and to make the probing process faster due to the lower number of features.

The final procedure is to do a quick exhaustive search for the subset of categories from which we can generate the best model out of. The result of this process as well as the result of the modelling process in general will be discussed in detail in section \@ref(rep). 

``` {r include = FALSE}
procedure <- read.csv(mp(DATA_MISC_DIR, "procedure.csv"), stringsAsFactors = FALSE)
```


```{r echo = FALSE, results = "asis", tidy = FALSE}
preprocess <- procedure[procedure$step == "modelling" & procedure$category == "modelling", ]
for(i in 1:nrow(preprocess)) {
  cat("> ",  preprocess$code[i], "\n\n")
  cat("```\n[",  preprocess$entity[i], "] ", preprocess$description[i], "\n", sep = "")
  cat("[ACTION] ", preprocess$action[i], "\n", sep = "")
  cat("[REASON] ", preprocess$reason[i], "\n```\n", sep = "")
  cat("\n")
}
```





## Classifiers and Tuning Parameters

Since this is a classification problem, tree-based and rule-based classifier naturally come to our mind. We will use 1R as our base classifier, and also try Naive Bayes as a representative of probability-based classifier. In terms of linear regression, we choose Neural Network, Support Vector Machine (SVM) and a simple modified version of linear regression that can work with nominal target. The modified version of linear regression involves choosing a threshold to decide the class of the target value. Regression tree, however, is not considered because the process of building the tree often looks for minimizing the variance at the leaves [@witten2017], which is in no way compatible with our typical performance metrics such as accuracy and Kappa value. Lastly, we also include some instance-based learners for the sake of completeness. In the following sub-sections, we will go into detail of each group of classifiers.






### Base Classifer

Classifier that is really simple or constructed based on naive assumption on the data set. They generally works decently in most cases so they can be used to produce lower bound for the performance of more sophisticated classifiers. A typical use of this lower bound is to detect drastic reduction in performance of other classifiers while tuning, which potentially suggests over-fitting.

#### 1R {-}
A simple rule-based classifier that only attempts to build 1 rule for the splits on one descriptive feature. It makes the assumption that there exists one feature that can be used to predict the target really well. As such, we can expect OneR to do well when we include strong predictor like the 2012 result. 1R serves well as the base classifier whose model's performance can serves as the lower bound for other more sophisticated models. 1R only works with nominal descriptive features (DF) and target feature (TF), so we need to bin our the numeric features in our data set. We, however, can use continuous features with the implementation RWeka::OneR as it has a built-in supervised discretization mechanism [@witten2017] which allows user to specify the minimum bucket size for discretization. We will, however, use the default supervised discretization function in RWeka developed using Fayyad & Irani's MDL method. Since this is just the base classifier, we will not spend a great deal of effort trying to tune it. 


> Summary

```
  Algorithm: OneR
  Package: RWeka
  Input Requirement:
      Descriptive Feature: nominal
      Target Feature: nominal
      Other: none
```

> Tuning Parameter

```
  Binning method (oner.bin)
  Number of bins (oner.nbin)
```





### Probability-based Classififer

Classifier that constructs the a set of conditional probability and priors for features value to calculate the value of the target feature. Usually, we will include Naive Bayes as a base classifier but we have done prior exploration and Naive Bayes yields amazing good results which makes us decide to separate it from 1R. Since the produced model is a set of probability, interpretation might be quite challenging.

#### Naive Bayes {-}
This is the only probabilistic method used. It takes the "naive" assumption that every features contribute equally to the result and it calculates the conditional probability of each feature to predict the value of the target feature. As such, the resulting model will be in the form of a set of table of probabilities for each feature. It works well with continuous DF but the worry that we have is that Naive Bayes assumes normal distribution for the continuous features so it can calculate the conditional probability of a novel instance using the probability density function. Most of our DFs have very skewed distributions, so we think it is best to force discretization on all features. As of course, similar to 1R, when it comes to discretization on heavily skewed data distribution, the best way is to avoid equal-interval discretization and to use either supervised binning or equal-frequency binning. The Naive Bayes implementation from e1071::NaiveBayes also supports Laplace smoothing (to avoid absolute 0 probability) so we can vary the value of this estimator in our tuning process.


> Summary

```
  Algorithm: NaiveBayes
  Package: e1071
  Input Requirement:
      Descriptive Feature: any
      Target Feature: nominal
      Other: none
```

> Tuning Parameter

```
  Binning method (nb.bin)
  Number of bins (nb.nbin)
  Laplace estimator (nb.laplace): a positive double controlling Laplace smoothing. 
                                  The default (0) disables Laplace smoothing.
```




### Rule-based Classifier

Classifiers that produce rule set: particularly, in this case, we have both PART and RIPPER producing decision lists (sets of ordered rules). Rules can be more comprehensive than most of the other models (arguably better than decision tree) so even if the result of models built by these classifiers are not the best, we can still based on them to learn about the data set.

#### PART {-}
A rule learning classifier that combines the divide-and-conquer strategy of decision-tree with separate-and-conquer strategy of rule learning. It builds partial (C4.5) decision tree on the current set of instances and makes the "best" leaf into a rule. The partial tree is then discarded and the instances covered by the rule is then removed. The process repeats until all training instances are classified. It works well with any type of DF. For this particular implementation RWeka::PART, we can change the minimum number of objects per leaf. We can also choose if we want to enable reduced-error pruning, otherwise, we can specify the confidence threshold for pruning, set by default at 25%, which works well in most case [@witten2017].


> Summary

```
  Algorithm: PART
  Package: RWeka
  Input Requirement:
      Descriptive Feature: any
      Target Feature: nominal
      Other: none
```

> Tuning Parameter

```
  Minimum number of objects per leaf (part.minLeaf)
  Enable reduced-error pruning (part.REP)
  Confidence threshold for pruning (part.pruneConf)
```

#### RIPPER {-}
Repeated Incremental Pruning to Produce Error Reduction (RIPPER) is a very popular rule extraction algorithm since it scales linearly with the number of instances and works well with imbalanced class distributions. It predicts the most frequent (majority) class and learns rules for the less frequent ones, starting with the least frequent. It uses the general to specific approach: growing one rule by greedily adding antecedents (or conditions) to the rule until the rule is perfect (i.e. 100% accurate). The procedure tries every possible value of each attribute and selects the condition with highest FOIL's information gain. It stops adding conjuncts if the number of negative examples covered increases. It uses reduced-error pruning to post-prune added conjuncts in reverse order. The specification of this algorithm is very similar to that of PART. However, for this implementation RWeka::Jrip, we can only modify the minimal weights of instances within a split.

> Summary

```
  Algorithm: JRip
  Package: RWeka
  Input Requirement:
      Descriptive Feature: any
      Target Feature: nominal
      Other: none
```

> Tuning Parameter

```
  Minimum weight of instance within a split (jrip.minWeight)
```





### Tree-based Classifier
The learning approach which builds decision tree as the model. It is as interpretable as rule-based models, but it has the advantage in that it is much easier to visualize. We choose the classic C4.5 algorithm and the CART algorithm. Tree-based classifiers like rule-based are non-parametric and thus does not assume any data distribution. As such, they are good choices for our data set.

#### C4.5 {-}
An improvement of ID3 algorithm as it uses gain ratio to determine which feature to split on instead of using information gain, which is very susceptible to features with high cardinality. It supports continuous features, as it looks for optimal threshold to split the instance: it sorts the instance by the value of the DF and considers the midpoint between target value change as potential split point, for each of which it calculates the gain ratio to choose the best split point. Like the PART implementation, RWeka::J48 allows us to choose if we want to enable reduced-error pruning as well as to modify the minimum number of objects per leaf.

> Summary

```
  Algorithm: J48
  Package: RWeka
  Input Requirement:
      Descriptive Feature: any
      Target Feature: nominal
      Other: none
```

> Tuning Parameter

```
  Minimum number of objects per leaf (j48.minLeaf)
  Enable reduced-error pruning (j48.REP)
  Confidence threshold for pruning (j48.pruneConf)
```


#### CART {-}
This is an algorithm that uses recursive partitioning for classification. This implementation rpart::rpart allows us to modify various parameters including the complexity parameter (cp). Any split that does not decrease the overall lack of fit by a factor of cp is not attempted. There are other approach offered by the implementation as an alternative to this early-stopping criteria such as changing number of observations that must exist in a node in order for a split to be attempted or the minimum number of observations in any terminal (leaf) node. But we feel that using cp is a characteristic value for this algorithm as we can either pre-prune or post-prune just by changing the the cp value.

> Summary

```
  Algorithm: rpart
  Package: rpart
  Input Requirement:
      Descriptive Feature: any
      Target Feature: nominal
      Other: none
```

> Tuning Parameter

```
  Complexity parameter (rpart.cp)
```





### Regression-based Classifier
Regression-based classifier attempts to put weight on each features to come up with a linear formula to compute the numeric value of the target. It can also be used for classification problem (e.g. logistic regression, supporting vector machines, etc.) to find the hyper-plane that separates the assumed linearly separable instance space. They general work well and rather robust, but some like neural network is too computationally expensive. Another downside of this approach is that for algorithm that only outputs numeric values for the target, we have to choose a threshold for the data to turn regression results into classification results, and lastly, the resulting models come as a set of weights, which is really hard to interpret. Since the underlying mechanism is to put weights on each feature, regression-based are susceptible to data with different scales so we will normalize or standardize all numeric features. Fortunately, binary features naturally fall into the range [0,1] like normalized features do.

#### Linear Regression {-}
The classifier we choose here is actually least-squares linear regression which attempts to minimize the sum of the squares of the differences over all the training instances by choosing weights for the linear combination of all features. For the native implementation of R, i.e. base::lm, we only tune the threshold which is used to determine the class of the predicted values. Since this algorithm only works for continuous features, we have to turn nominal features into binary, i.e. the state FIPS code must be turned into multiple binary features. The downside of this, as we have discovered is that the learned model does not end up using all of the features, resulting in many warnings from R with "rank-deficient fit." Essentially it means that one or more feature can be be expressed as the linear combination of other features so they are discarded while building the model. There is also a popular misconception that linear model is parametric, which means they assume normal distribution of the input. But as pointed out, the assumption on distribution only applies to the error, not the data itself [@williams2013], so we can safely use linear regression without worrying to much about transforming the distribution of the features.

> Summary

```
  Algorithm: lm
  Package: base
  Input Requirement:
      Descriptive Feature: continuous
      Target Feature: continuous
      Other: normalize
```

> Tuning Parameter

```
  Decision threshold (lm.threshold)
```


#### Support Vector Machines (SVM) {-}
The algorithm tries to find the best hyper-plane that separates assumed linearly separable instance space. The implementation e1071:svm supports classification and allows us to modify the kernel used in training and predicting as well as cost of constraints violation, i. e. the _C_-constant of the regularization term in the Lagrange formulation. In essence, the constant tells SVM optimization how much we want to avoid mis-classifying each training instances. For large values of C, the optimization will choose a smaller-margin hyper-plane if that hyper-plane does a better job of getting all the training points classified correctly. Conversely, a very small value of C will cause the optimizer to look for a larger-margin separating hyper-plane, even if that hyper-plane mis-classifies more points. Since our knowledge on the various kernel type is limited, we will not attempt to tune on the parameters that correspond to each kernel type but using the default values instead. Another issue is that we have to scale/standardize the features since it helps "avoid attributes in greater numeric ranges dominating those in smaller numeric ranges and numerical difficulties during the calculation" as "kernel values usually depend on the inner products of feature vectors, e.g. the linear kernel and the polynomial kernel, large attribute values might cause numerical problems" [@hsu2003]. Since SVM is sensitive to imbalanced class distribution, we will also adjust the weights if training set is left unsampled by adjusting the parameter class.weights in the e1071::svm implementation.

> Summary

```
  Algorithm: svm
  Package: e1071
  Input Requirement:
      Descriptive Feature: continuous
      Target Feature: any
      Other: standardize
```

> Tuning Parameter

```
  Kernel type (svm.kernel)
  Cost of constraints violation (svm.cost)
```


#### Neural Network {-}
This is probably the most advanced (as well as computationally expensive) algorithm used in this study. Neural Network seeks to chain up multiple layers of perceptron/components that assign weights to its inputs for computing outputs, in order to optimize the performance. As such, in terms of tuning, for this particular implementation neuralnet::neuralnet, we have the freedom to choose the shape of the hidden layers. As more sophisticated configurations (2 layers or more) often require more time, we have to raise the initial limit on the maximum number of step from 100000 to 10000000 to make sure the algorithm converges. We decide not to try beyong 3 layers as we have tested with 3 hidden layers and this take brutally long to finish and we can see that those complicated neural network do not always perform better. Also because 2 layers are often sufficient to generalize well [@witten2017] (though again, data mining in general tackles ill-posed problems, there is no guaranty that a model really expresses the true underlying function), and 1 layer with 1 node merely resembles the structure of linear regression approach, we will tune in the range from 1-hidden-layer with multiple nodes to 2-hidden-layer with various node configurations. The implementation also allows modifying the algorithm. The default is RPROP, with 2 flavors rprop+ and rprop-, which stands for resilient backpropagation with and without weight backtracking. We have tried to work with the normal backpropagation method but this requires specifying the learning rate which differs in each case. Resilient backpropagation modifies the learning rate automatically so we can avoid getting stuck due to inappropriate learning rate assignment. Last but not least, since neural network outputs numeric values, we set the decision threshold to 0.5.

> Summary

```
  Algorithm: neuralnet
  Package: neuralnet
  Input Requirement:
      Descriptive Feature: continuous
      Target Feature: continuous
      Other: normalize
```

> Tuning Parameter

```
  Algorithm (neuralnet.algorithm)
  Hidden layer(s) configuration (neuralnet.hidden)
```





### Instance-based Classifier
This class of classifiers are often called lazy-learners since they store the training set as the model and predict novel data by comparing them to the existing training instances. Here, we choose to use K-Nearest Neighbor which will determine the class value of the novel instances based on its k nearest instances in the instance storing data structure. Since the whole idea of the algorithm is to find the closest neighbors, it then must have a distance metric. This works best when all features are of numeric type (nominal features result in very crude distance measure such as 0 or 1) and when they have appropriate scales [@witten2017]. As such, it is best to normalize all numeric features.

#### K-Nearest Neighbor {-}
For this particular implementation class:knn, we can only modify the number of neighbor k. However, there is a [known bug](http://r.789695.n4.nabble.com/error-in-knn-too-many-ties-in-knn-td3798689.html), which limits the size of k to less than 500. Of course, when we increase k to 500, that pretty much defeats the purpose of this algorithm, we want to find the optimal value of k that is not so large (to make classification works relatively fast) but still large enough to create model that performs well. Another consideration that worth mentioning is that for unweighted implementation, for a 2-class problem, even value of k often yields less optimal result because ties in vote can occur, as such, we can optimize our tuning process by just considering odd values of k.


> Summary

```
  Algorithm: knn
  Package: class
  Input Requirement:
      Descriptive Feature: continuous
      Target Feature: nominal
      Other: normalize
```

> Tuning Parameter

```
  Number of nearest neighbor (knn.k)
```


#### Weighted K-Nearest Neighbor {-}
This is the weighted version of the basic K-Nearest Neighbor algorithm. This implementation kknn::kknn allows us to modify not only the number k of neighbors but also the kernel type used and the distance metrics. Despite this degree of freedom, we choose not to tune these 2 values but to use the default because this particular algorithm is rather computationally expensive and we have to tune for the best k value in a wide range of k. The default metric distance is 2, which is equivalent to Euclidean metric in Minkowski distance class; this to us is a fair choice. As for the k parameter, as each neighbor now weighs differently, we have to consider even value of k as well.

> Summary

```
  Algorithm: kknn
  Package: kknn
  Input Requirement:
      Descriptive Feature: continuous
      Target Feature: nominal
      Other: normalize
```

> Tuning Parameter

```
  Number of nearest neighbor (kknn.k)
```

## Addressing Imbalanced Class Distribution
As noted before in section \@ref(dqr-exploration), we have an imbalanced class distribution for our target feature. This might hurt some of the classifiers we are about to use. This imbalance affects the estimate of the intercept of linear models [@king2001]. It also affects tree-based model [@cieslak2008] and neural network unless cost-sensitivity is introduced to the learning algorithms [@he2009]. As such, during the training process, we will proceed with either under-sampling or over-sampling. Nevertheless, we will still try our luck with the original data as we know some classifiers we use also deal well with imbalanced distribution, i.e. rule-based classifiers such as RIPPER, since they employ the separate-and-conquer approach which only targets at only one class at a time. PART is an exception here as it attempts to build the partial tree first so it might be at a disadvantage while working with imbalanced dataset. The SVM algorithm we use is from the package e1071 which uses the engine of [LIBSVM](https://www.csie.ntu.edu.tw/~cjlin/libsvm/), which supports weighted SVM for unbalanced data. 

Each sampling strategy, however, has its own quirks. Under-sampling requires us to remove instances of the majority class to match the number of instances of the minority class. Throwing away real data is never a great option. In turn, the size of the data set shrinks and this costs models learned using this training set more susceptible to the effect of the random split. So we have to increase the number of fold for cross-validation. Over-sampling, on the other hands, involves boostraping data with replacement to boost the number of minority class instances to match that of the majority class. Duplicating data is also not a great option neither since it randomly duplicates __the minority data instances.__ This does not gain us any information, but also potentially leads to over-fitting [@ganganwar2012]. This also significantly increases the size of the data set, making computation slower and thus, we decide to reduce the number of fold in cross validation. Over-sampling has a slight advantage over undersampling in justifying for our reducing number of fold in cross validation in that it has bigger training set, and no instance is thrown away, as such, it might have slightly better performance. Nevertheless, for both under-sampling and over-sampling, we risk increasing the effect of outliers. These trade-offs are clearly significant and we cannot really tell which is straight-out better than the other. As such, in the evaluation phase, we will compare these on case by case basis.